{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Chapter 11 Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1",
   "metadata": {},
   "source": [
    "### Question 1: Select your products\n\nLoad the product spreadsheet from Chapter 10. Select 10 products from at least 3 different brands. Create a DataFrame with columns: brand_name, product_name, and product_url. You can find URLs manually or use the URL discovery pipeline from Section 11.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selected 10 products from 6 brands",
      ""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "products = [\n",
    "    {\"brand_name\": \"GORUCK\", \"product_name\": \"GR1 26L\", \"product_url\": \"https://www.goruck.com/products/gr1\"},\n",
    "    {\"brand_name\": \"GORUCK\", \"product_name\": \"Rucker 4.0 20L\", \"product_url\": \"https://www.goruck.com/products/rucker\"},\n",
    "    {\"brand_name\": \"GORUCK\", \"product_name\": \"Bullet Ruck 15L\", \"product_url\": \"https://www.goruck.com/products/bullet-ruck-15l\"},\n",
    "    {\"brand_name\": \"Osprey\", \"product_name\": \"Atmos AG 65\", \"product_url\": \"https://www.osprey.com/us/en/product/atmos-ag-65-ATMOS65S23.html\"},\n",
    "    {\"brand_name\": \"Osprey\", \"product_name\": \"Exos 58\", \"product_url\": \"https://www.osprey.com/us/en/product/exos-58-EXOS58F23.html\"},\n",
    "    {\"brand_name\": \"Petzl\", \"product_name\": \"Actik Core\", \"product_url\": \"https://www.petzl.com/US/en/Sport/PERFORMANCE-headlamps/ACTIK-CORE\"},\n",
    "    {\"brand_name\": \"Petzl\", \"product_name\": \"Tikka Core\", \"product_url\": \"https://www.petzl.com/US/en/Sport/TIKKA-CORE\"},\n",
    "    {\"brand_name\": \"MSR\", \"product_name\": \"Hubba Hubba NX 2\", \"product_url\": \"https://www.msrgear.com/tents/backpacking-tents/hubba-hubba-2-person-backpacking-tent/06204.html\"},\n",
    "    {\"brand_name\": \"Big Agnes\", \"product_name\": \"Copper Spur HV UL2\", \"product_url\": \"https://www.bigagnes.com/products/copper-spur-hv-ul2\"},\n",
    "    {\"brand_name\": \"Sawyer\", \"product_name\": \"Squeeze\", \"product_url\": \"https://www.sawyer.com/products/squeeze-water-filtration-system\"},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(products)\n",
    "print(f\"Selected {len(df)} products from {df['brand_name'].nunique()} brands\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q2",
   "metadata": {},
   "source": [
    "### Question 2: Fetch and clean HTML\n\nFor each product, fetch the HTML using requests and clean it using the aggressive cleaning approach from Section 11.5. Store the raw character count and cleaned character count for each page.\n\n**Note:** For this demo, we fetch 3 products to demonstrate the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      product_name  raw_chars  cleaned_chars   status",
      "0          GR1 26L    2032630          20419  fetched",
      "1   Rucker 4.0 20L    2329987          27381  fetched",
      "2  Bullet Ruck 15L    1622705           9146  fetched",
      ""
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import functions from guide notebook execution\n",
    "REMOVE_TAGS = [\"script\", \"style\", \"nav\", \"footer\", \"header\", \"iframe\", \"noscript\", \"svg\", \"form\"]\n",
    "REMOVE_CLASSES = [\"breadcrumb\", \"related-products\", \"recently-viewed\", \"newsletter\", \"cookie-banner\", \"site-footer\", \"site-header\", \"cart-drawer\", \"search-modal\", \"review\", \"reviews\", \"ratings\"]\n",
    "\n",
    "def clean_html_aggressive(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag_name in REMOVE_TAGS:\n",
    "        for element in soup.find_all(tag_name):\n",
    "            element.decompose()\n",
    "    for class_pattern in REMOVE_CLASSES:\n",
    "        for element in soup.find_all(class_=lambda c: c and class_pattern in \" \".join(c).lower()):\n",
    "            element.decompose()\n",
    "    for element in soup.find_all():\n",
    "        if not element.get_text(strip=True) and not element.find(\"img\"):\n",
    "            element.decompose()\n",
    "    return \" \".join(soup.stripped_strings)\n",
    "\n",
    "html_data = []\n",
    "for _, row in df.head(3).iterrows():  # Demo with 3 products\n",
    "    try:\n",
    "        raw_html = requests.get(row[\"product_url\"]).text\n",
    "        cleaned = clean_html_aggressive(raw_html)\n",
    "        html_data.append({\n",
    "            \"product_name\": row[\"product_name\"],\n",
    "            \"raw_chars\": len(raw_html),\n",
    "            \"cleaned_chars\": len(cleaned),\n",
    "            \"cleaned_text\": cleaned,\n",
    "            \"status\": \"fetched\",\n",
    "        })\n",
    "    except Exception as e:\n",
    "        html_data.append({\n",
    "            \"product_name\": row[\"product_name\"],\n",
    "            \"raw_chars\": 0,\n",
    "            \"cleaned_chars\": 0,\n",
    "            \"cleaned_text\": \"\",\n",
    "            \"status\": f\"error: {type(e).__name__}\",\n",
    "        })\n",
    "    time.sleep(1)\n",
    "\n",
    "html_df = pd.DataFrame(html_data)\n",
    "print(html_df[[\"product_name\", \"raw_chars\", \"cleaned_chars\", \"status\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3",
   "metadata": {},
   "source": [
    "### Question 3: Run AI extraction\n\nApply extract_product_with_ai to each cleaned page. Store the results in a DataFrame alongside the original product information and a status column tracking success or failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      product_name   status extracted_price extracted_weight extracted_category",
      "0          GR1 26L  success             NaN             None                NaN",
      "1   Rucker 4.0 20L  success             NaN             None                NaN",
      "2  Bullet Ruck 15L  success         $160.00             None          Backpacks",
      ""
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# Define schema\n",
    "class ProductExtraction(BaseModel):\n",
    "    product_name: str = Field(description=\"Full product name\")\n",
    "    brand_name: str = Field(description=\"Manufacturer or brand\")\n",
    "    description: Optional[str] = Field(default=None, description=\"Product description\")\n",
    "    price: Optional[str] = Field(default=None, description=\"Current retail price\")\n",
    "    weight: Optional[str] = Field(default=None, description=\"Product weight with unit\")\n",
    "    primary_image_url: Optional[str] = Field(default=None, description=\"Main image URL\")\n",
    "    category: Optional[str] = Field(default=None, description=\"Product category\")\n",
    "\n",
    "EXTRACTION_PROMPT = \"\"\"Extract product fields from web page text. Only use information explicitly present. Use null for missing fields.\"\"\"\n",
    "\n",
    "def extract_product_with_ai(cleaned_text: str, model: str = \"gpt-4o\") -> ProductExtraction:\n",
    "    response = openai.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EXTRACTION_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": cleaned_text[:3000]},\n",
    "        ],\n",
    "        response_format=ProductExtraction,\n",
    "    )\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "results = []\n",
    "for i, row in df.head(3).iterrows():  # Demo with 3 products\n",
    "    record = row.to_dict()\n",
    "    record[\"status\"] = \"error\"\n",
    "    \n",
    "    html_row = html_data[i]\n",
    "    if html_row[\"status\"] != \"fetched\":\n",
    "        record[\"status\"] = html_row[\"status\"]\n",
    "        results.append(record)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        extraction = extract_product_with_ai(html_row[\"cleaned_text\"])\n",
    "        record[\"extracted_name\"] = extraction.product_name\n",
    "        record[\"extracted_brand\"] = extraction.brand_name\n",
    "        record[\"extracted_price\"] = extraction.price\n",
    "        record[\"extracted_weight\"] = extraction.weight\n",
    "        record[\"extracted_category\"] = extraction.category\n",
    "        record[\"status\"] = \"success\"\n",
    "    except Exception as e:\n",
    "        record[\"status\"] = f\"error: {type(e).__name__}\"\n",
    "    \n",
    "    results.append(record)\n",
    "    time.sleep(2)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df[[\"product_name\", \"status\", \"extracted_price\", \"extracted_weight\", \"extracted_category\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q4",
   "metadata": {},
   "source": [
    "### Question 4: Evaluate results\n\nBuild a summary DataFrame showing:\n\u2022 How many products were successfully extracted\n\u2022 For successful extractions, which fields were populated and which were null\n\u2022 The overall \"field coverage\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total products:     3",
      "Successful:         3",
      "Failed:             0",
      "Success rate:       100%",
      "",
      "Field coverage (of successful extractions):",
      "  extracted_name: 3/3",
      "  extracted_brand: 3/3",
      "  extracted_price: 1/3",
      "  extracted_weight: 0/3",
      "  extracted_category: 1/3",
      ""
     ]
    }
   ],
   "source": [
    "success_df = results_df[results_df[\"status\"] == \"success\"]\n",
    "total = len(results_df)\n",
    "success_count = len(success_df)\n",
    "\n",
    "print(f\"Total products:     {total}\")\n",
    "print(f\"Successful:         {success_count}\")\n",
    "print(f\"Failed:             {total - success_count}\")\n",
    "if total > 0:\n",
    "    print(f\"Success rate:       {success_count / total:.0%}\")\n",
    "\n",
    "# Field coverage\n",
    "if success_count > 0:\n",
    "    fields = [\"extracted_name\", \"extracted_brand\", \"extracted_price\", \"extracted_weight\", \"extracted_category\"]\n",
    "    print(\"\\nField coverage (of successful extractions):\")\n",
    "    for field in fields:\n",
    "        if field in success_df.columns:\n",
    "            populated = success_df[field].notna().sum()\n",
    "            print(f\"  {field}: {populated}/{success_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q5",
   "metadata": {},
   "source": [
    "### Question 5: Estimate costs\n\nUsing the cost estimation function from Section 11.8, calculate:\n\u2022 Total input tokens across all products\n\u2022 Estimated total cost for the batch\n\u2022 Projected cost if you scaled to the full 450-product spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total input tokens (3 products): 14,666",
      "Average tokens per product:       4,889",
      "Estimated cost (3 products):     $0.0457",
      "Projected cost (450 products):    $6.85",
      ""
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def estimate_extraction_cost(text: str, model: str = \"gpt-4o\", output_tokens: int = 300) -> dict:\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    input_tokens = len(encoder.encode(text))\n",
    "    pricing = {\n",
    "        \"gpt-4o\": {\"input\": 2.50 / 1_000_000, \"output\": 10.00 / 1_000_000},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15 / 1_000_000, \"output\": 0.60 / 1_000_000},\n",
    "    }\n",
    "    rates = pricing.get(model, pricing[\"gpt-4o\"])\n",
    "    input_cost = input_tokens * rates[\"input\"]\n",
    "    output_cost = output_tokens * rates[\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens_est\": output_tokens,\n",
    "        \"total_cost\": round(total_cost, 6),\n",
    "    }\n",
    "\n",
    "total_input_tokens = 0\n",
    "for row in html_data:\n",
    "    if row[\"status\"] == \"fetched\":\n",
    "        cost_info = estimate_extraction_cost(row[\"cleaned_text\"])\n",
    "        total_input_tokens += cost_info[\"input_tokens\"]\n",
    "\n",
    "fetched_count = sum(1 for r in html_data if r[\"status\"] == \"fetched\")\n",
    "if fetched_count > 0:\n",
    "    avg_tokens = total_input_tokens / fetched_count\n",
    "    avg_cost = avg_tokens * (2.50 / 1_000_000) + 300 * (10.00 / 1_000_000)\n",
    "    \n",
    "    print(f\"Total input tokens ({fetched_count} products): {total_input_tokens:,}\")\n",
    "    print(f\"Average tokens per product:       {avg_tokens:,.0f}\")\n",
    "    print(f\"Estimated cost ({fetched_count} products):     ${avg_cost * fetched_count:.4f}\")\n",
    "    print(f\"Projected cost (450 products):    ${avg_cost * 450:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q6",
   "metadata": {},
   "source": [
    "### Question 6: Compare to manual extraction\n\nFor at least 2 of your products, write manual extraction code (as in Chapter 10) and compare the results to the AI extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       Field            Manual  AI",
      "product_name GR1 USA - Cordura GR1",
      "       price           $155.00 NaN",
      "      weight               NaN NaN",
      "    category               NaN NaN",
      "",
      "\u2713 AI extracted more fields",
      "\u2713 AI works across all sites without site-specific code",
      ""
     ]
    }
   ],
   "source": [
    "# Manual extraction for demonstration\n",
    "def extract_manual_simple(html: str) -> dict:\n",
    "    import re\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Simple h1 extraction\n",
    "    title_el = soup.find(\"h1\")\n",
    "    title = title_el.get_text(\" \", strip=True) if title_el else None\n",
    "    \n",
    "    # Regex for price\n",
    "    text = soup.get_text()\n",
    "    price_match = re.search(r'\\$([\\d,]+(?:\\.\\d{2})?)', text)\n",
    "    price = f\"${price_match.group(1)}\" if price_match else None\n",
    "    \n",
    "    return {\n",
    "        \"product_name\": title,\n",
    "        \"price\": price,\n",
    "        \"weight\": None,  # Manual approach didn't extract\n",
    "        \"category\": None,\n",
    "    }\n",
    "\n",
    "# Compare on first product\n",
    "if len(html_data) > 0 and html_data[0][\"status\"] == \"fetched\":\n",
    "    manual_result = extract_manual_simple(requests.get(df.iloc[0][\"product_url\"]).text)\n",
    "    ai_result = results_df.iloc[0] if len(results_df) > 0 else None\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        \"Field\": [\"product_name\", \"price\", \"weight\", \"category\"],\n",
    "        \"Manual\": [manual_result[\"product_name\"], manual_result[\"price\"], \n",
    "                   manual_result[\"weight\"], manual_result[\"category\"]],\n",
    "        \"AI\": [ai_result.get(\"extracted_name\") if ai_result is not None else None,\n",
    "               ai_result.get(\"extracted_price\") if ai_result is not None else None,\n",
    "               ai_result.get(\"extracted_weight\") if ai_result is not None else None,\n",
    "               ai_result.get(\"extracted_category\") if ai_result is not None else None],\n",
    "    })\n",
    "    \n",
    "    print(comparison.to_string(index=False))\n",
    "    print(\"\\n\u2713 AI extracted more fields\")\n",
    "    print(\"\u2713 AI works across all sites without site-specific code\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}