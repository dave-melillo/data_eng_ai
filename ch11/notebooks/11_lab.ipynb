{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Chapter 11 Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"\u2713 Environment variables loaded\")\n",
    "print(\"Ready to run lab exercises\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1",
   "metadata": {},
   "source": [
    "### Question 1: Load curated products\\n\\nLoad the curated backpack list that has been pre-tested for reliable extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"QUESTION 1: Load Curated Products\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load the curated backpack list (pre-tested for reliable extraction)\n",
    "df = pd.read_csv(\"../data/curated_backpacks.csv\")\n",
    "\n",
    "print(f\"\\nLoaded {len(df)} products from {df['Brand Name'].nunique()} brands\\n\")\n",
    "display(df)\n",
    "print(f\"\\n\u2713 Product selection complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q2",
   "metadata": {},
   "source": [
    "### Question 2: URL Discovery\n",
    "\n",
    "Use SerpAPI to search for product URLs (Listing 11.1) and AI ranking (Listing 11.2) to select the best URL for each product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"QUESTION 2: URL Discovery & Ranking\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cleaning functions from guide\n",
    "REMOVE_TAGS = [\"script\", \"style\", \"nav\", \"footer\", \"header\", \"iframe\", \"noscript\", \"svg\", \"form\"]\n",
    "REMOVE_CLASSES = [\"breadcrumb\", \"related-products\", \"recently-viewed\", \"newsletter\", \"cookie-banner\", \"site-footer\", \"site-header\", \"cart-drawer\", \"search-modal\", \"review\", \"reviews\", \"ratings\"]\n",
    "\n",
    "def clean_html_aggressive(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in REMOVE_TAGS:\n",
    "        for el in soup.find_all(tag):\n",
    "            el.decompose()\n",
    "    for class_pattern in REMOVE_CLASSES:\n",
    "        for el in soup.find_all(class_=lambda c: c and class_pattern in \" \".join(c).lower()):\n",
    "            el.decompose()\n",
    "    for el in soup.find_all():\n",
    "        if not el.get_text(strip=True) and not el.find(\"img\"):\n",
    "            el.decompose()\n",
    "    return \" \".join(soup.stripped_strings)\n",
    "\n",
    "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")\n",
    "\n",
    "def search_product_url(brand: str, product: str) -> str:\n",
    "    \"\"\"Search for a product URL using SerpAPI.\"\"\"\n",
    "    search_key = f\"{brand} {product}\"\n",
    "    params = {\"q\": search_key, \"api_key\": SERPAPI_KEY, \"num\": 5, \"engine\": \"google\"}\n",
    "    resp = requests.get(\"https://serpapi.com/search\", params=params)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    \n",
    "    # Return first result URL (AI ranking can be added for production)\n",
    "    return results[0].get(\"link\", \"\") if results else \"\"\n",
    "\n",
    "print(f\"\\nFetching & cleaning HTML for {len(df)} products...\\n\")\n",
    "\n",
    "html_data = []\n",
    "for _, row in df.iterrows():\n",
    "    brand = row[\"Brand Name\"]\n",
    "    product = row[\"Product Name\"]\n",
    "    \n",
    "    try:\n",
    "        print(f\"  \u2022 {brand} {product}...\")\n",
    "        \n",
    "        # Search for product URL\n",
    "        url = search_product_url(brand, product)\n",
    "        if not url:\n",
    "            html_data.append({\n",
    "                \"product_name\": product,\n",
    "                \"brand_name\": brand,\n",
    "                \"status\": \"no_url\",\n",
    "                \"cleaned_text\": \"\",\n",
    "                \"url_used\": \"\",\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Fetch and clean\n",
    "        raw_html = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"}).text\n",
    "        cleaned = clean_html_aggressive(raw_html)\n",
    "        \n",
    "        html_data.append({\n",
    "            \"product_name\": product,\n",
    "            \"brand_name\": brand,\n",
    "            \"raw_chars\": len(raw_html),\n",
    "            \"cleaned_chars\": len(cleaned),\n",
    "            \"reduction_pct\": round((1 - len(cleaned) / len(raw_html)) * 100, 1),\n",
    "            \"cleaned_text\": cleaned,\n",
    "            \"status\": \"fetched\" if len(cleaned) > 500 else \"low_content\",\n",
    "            \"url_used\": url,\n",
    "        })\n",
    "    return candidates\n",
    "\n",
    "# Listing 11.2: Rank URLs with AI\n",
    "class URLRanking(BaseModel):  #E\n",
    "    best_url: str\n",
    "    confidence: str\n",
    "    reasoning: str\n",
    "\n",
    "def rank_urls_with_ai(search_key: str, candidates: list[dict], model: str = \"gpt-4o-mini\") -> URLRanking:  #F\n",
    "    \"\"\"Use an LLM to pick the best product page from search results.\"\"\"\n",
    "    candidate_text = \"\"\n",
    "    for c in candidates:\n",
    "        candidate_text += (\n",
    "            f\"Position {c['position']}:\\n\"\n",
    "            f\"  Title: {c['title']}\\n\"\n",
    "            f\"  URL: {c['url']}\\n\"\n",
    "            f\"  Snippet: {c['snippet']}\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    system_prompt = \"\"\"You are a data engineering assistant helping build a product database.\n",
    "Given a product search key and a list of candidate URLs from search results,\n",
    "pick the single best URL for extracting structured product data.\n",
    "\n",
    "Prefer:\n",
    "1. Manufacturer or official brand pages\n",
    "2. Major retailer pages (REI, Backcountry, Moosejaw) if manufacturer page unavailable\n",
    "3. Pages likely to contain: product name, price, description, weight, images\n",
    "4. Individual product pages over category or listing pages\n",
    "\n",
    "Avoid:\n",
    "- Review sites, forums, Reddit threads\n",
    "- Category pages that list multiple products\n",
    "\n",
    "Return the best URL, your confidence level, and a brief explanation.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"Product: {search_key}\\n\\nCandidate URLs:\\n{candidate_text}\"\n",
    "    \n",
    "    response = openai.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        response_format=URLRanking,\n",
    "    )\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "# Process each product\n",
    "print(f\"\\nDiscovering URLs for {len(df)} products...\\n\")\n",
    "\n",
    "url_data = []  #G\n",
    "for _, row in df.iterrows():\n",
    "    search_key = f\"{row['Brand Name']} {row['Product Name']}\"\n",
    "    print(f\"  \u2022 {search_key}...\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Search for URLs\n",
    "        candidates = search_product_urls(search_key)\n",
    "        \n",
    "        # Step 2: Rank URLs with AI\n",
    "        if candidates:\n",
    "            ranking = rank_urls_with_ai(search_key, candidates)\n",
    "            url_data.append({\n",
    "                \"brand_name\": row['Brand Name'],\n",
    "                \"product_name\": row['Product Name'],\n",
    "                \"search_key\": search_key,\n",
    "                \"url\": ranking.best_url,\n",
    "                \"confidence\": ranking.confidence,\n",
    "                \"status\": \"success\"\n",
    "            })\n",
    "        else:\n",
    "            url_data.append({\n",
    "                \"brand_name\": row['Brand Name'],\n",
    "                \"product_name\": row['Product Name'],\n",
    "                \"search_key\": search_key,\n",
    "                \"url\": None,\n",
    "                \"confidence\": None,\n",
    "                \"status\": \"no_results\"\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {type(e).__name__}\")\n",
    "        html_data.append({\n",
    "            \"product_name\": product,\n",
    "            \"brand_name\": brand,\n",
    "            \"status\": f\"error: {type(e).__name__}\",\n",
    "            \"cleaned_text\": \"\",\n",
    "            \"url_used\": \"\",\n",
    "        })\n",
    "    \n",
    "    time.sleep(1)  # Rate limiting\n",
    "\n",
    "html_df = pd.DataFrame(html_data)\n",
    "print(f\"\\nHTML Fetching & Cleaning Results:\\n\")\n",
    "display(html_df[[\"product_name\", \"brand_name\", \"status\", \"cleaned_chars\", \"reduction_pct\"]].head(10))\n",
    "\n",
    "success_count = (df_urls['status'] == 'success').sum()\n",
    "print(f\"\\n\u2713 {success_count}/{len(df_urls)} URLs discovered successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3",
   "metadata": {},
   "source": [
    "### Question 3: HTML Cleaning & AI Extraction\n",
    "\n",
    "Fetch HTML, apply aggressive cleaning (Listing 11.3), and extract product data with AI (Listing 11.6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pydantic import Field\n",
    "from typing import Optional\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"QUESTION 3: HTML Cleaning & AI Extraction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Listing 11.3: Aggressive HTML cleaning\n",
    "REMOVE_TAGS = [\"script\", \"style\", \"nav\", \"footer\", \"header\", \"iframe\", \"noscript\", \"svg\", \"form\"]  #I\n",
    "REMOVE_CLASSES = [\"breadcrumb\", \"related-products\", \"recently-viewed\", \"newsletter\", \"cookie-banner\", \"site-footer\", \"site-header\", \"cart-drawer\", \"search-modal\", \"review\", \"reviews\", \"ratings\"]\n",
    "\n",
    "def clean_html_aggressive(html: str) -> str:  #J\n",
    "    \"\"\"Remove non-product HTML elements to reduce noise and token count.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    for tag_name in REMOVE_TAGS:\n",
    "        for element in soup.find_all(tag_name):\n",
    "            element.decompose()\n",
    "    \n",
    "    for class_pattern in REMOVE_CLASSES:\n",
    "        for element in soup.find_all(class_=lambda c: c and class_pattern in \" \".join(c).lower()):\n",
    "            element.decompose()\n",
    "    \n",
    "    for element in soup.find_all():\n",
    "        if not element.get_text(strip=True) and not element.find(\"img\"):\n",
    "            element.decompose()\n",
    "    \n",
    "    return \" \".join(soup.stripped_strings)\n",
    "\n",
    "# Listing 11.5 & 11.6: Define schema and extraction function\n",
    "class ProductExtraction(BaseModel):  #K\n",
    "    product_name: str = Field(description=\"Full product name\")\n",
    "    brand_name: str = Field(description=\"Manufacturer or brand\")\n",
    "    description: Optional[str] = Field(default=None, description=\"Product description\")\n",
    "    price: Optional[str] = Field(default=None, description=\"Current retail price\")\n",
    "    weight: Optional[str] = Field(default=None, description=\"Product weight with unit\")\n",
    "    primary_image_url: Optional[str] = Field(default=None, description=\"Main image URL\")\n",
    "    category: Optional[str] = Field(default=None, description=\"Product category\")\n",
    "\n",
    "EXTRACTION_PROMPT = \"\"\"You are a product data extraction assistant for a data engineering pipeline.\n",
    "\n",
    "Given the text content of a product web page, extract the following fields accurately:\n",
    "- product_name: The full product name as displayed on the page\n",
    "- brand_name: The manufacturer or brand\n",
    "- description: A concise product description (1-3 sentences)\n",
    "- price: The current retail price with currency symbol\n",
    "- weight: The product weight with unit if available\n",
    "- primary_image_url: The URL of the main product image if found in the text\n",
    "- category: The product category (backpack, tent, sleeping bag, headlamp, etc.)\n",
    "\n",
    "Rules:\n",
    "- Only extract information that is explicitly present in the text\n",
    "- Use null for any field you cannot find or confidently determine\n",
    "- Do not guess or fabricate values\n",
    "- For price, use the current or sale price, not the original price if both are shown\n",
    "- For weight, include the unit (lbs, oz, kg, g)\n",
    "- For category, use a simple label based on what the product is\"\"\"\n",
    "\n",
    "def extract_product_with_ai(cleaned_text: str, model: str = \"gpt-4o\") -> ProductExtraction:  #L\n",
    "    \"\"\"Extract product fields from cleaned page text using an LLM.\"\"\"\n",
    "    response = openai.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EXTRACTION_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": cleaned_text[:8000]},\n",
    "        ],\n",
    "        response_format=ProductExtraction,\n",
    "    )\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "# Process successful URL discoveries\n",
    "print(f\"\\nProcessing {len(df_urls[df_urls['status'] == 'success'])} products...\\n\")\n",
    "\n",
    "extraction_results = []  #M\n",
    "for _, row in df_urls[df_urls['status'] == 'success'].iterrows():\n",
    "    record = {\n",
    "        \"brand_name\": row['brand_name'],\n",
    "        \"product_name\": row['product_name'],\n",
    "        \"url\": row['url'],\n",
    "        \"status\": \"error\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"  \u2022 {row['product_name']}...\")\n",
    "        \n",
    "        # Fetch & clean HTML\n",
    "        raw_html = requests.get(row['url'], timeout=10).text\n",
    "        cleaned = clean_html_aggressive(raw_html)\n",
    "        \n",
    "        # Extract with AI\n",
    "        extraction = extract_product_with_ai(cleaned)\n",
    "        \n",
    "        record.update({\n",
    "            \"extracted_name\": extraction.product_name,\n",
    "            \"extracted_brand\": extraction.brand_name,\n",
    "            \"extracted_price\": extraction.price,\n",
    "            \"extracted_weight\": extraction.weight,\n",
    "            \"extracted_category\": extraction.category,\n",
    "            \"raw_chars\": len(raw_html),\n",
    "            \"cleaned_chars\": len(cleaned),\n",
    "            \"status\": \"success\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"    Error: {type(e).__name__}\")\n",
    "        record[\"status\"] = f\"error: {type(e).__name__}\"\n",
    "    \n",
    "    extraction_results.append(record)\n",
    "    time.sleep(2)  # Rate limiting for API calls\n",
    "\n",
    "results_df = pd.DataFrame(extraction_results)  #N\n",
    "print(f\"\\nExtraction Results:\\n\")\n",
    "display(results_df[['product_name', 'status', 'extracted_price', 'extracted_weight', 'extracted_category']])\n",
    "\n",
    "success_count = (results_df['status'] == 'success').sum()\n",
    "print(f\"\\n\u2713 {success_count}/{len(results_df)} extractions successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q4",
   "metadata": {},
   "source": [
    "### Question 4: Evaluate Results\n",
    "\n",
    "Analyze extraction success rate and field coverage across all processed products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"QUESTION 4: Evaluate Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    success_df = results_df[results_df['status'] == 'success']\n",
    "    total = len(results_df)\n",
    "    success_count = len(success_df)\n",
    "    \n",
    "    # Summary stats\n",
    "    print(f\"\\nExtraction Summary:\\n\")\n",
    "    summary = pd.DataFrame({\n",
    "        \"Metric\": [\"Total products\", \"Successful\", \"Failed\", \"Success rate\"],\n",
    "        \"Value\": [\n",
    "            total,\n",
    "            success_count,\n",
    "            total - success_count,\n",
    "            f\"{success_count / total:.0%}\" if total > 0 else \"N/A\"\n",
    "        ]\n",
    "    })\n",
    "    display(summary)\n",
    "    \n",
    "    # Field coverage\n",
    "    if success_count > 0:\n",
    "        fields = ['extracted_name', 'extracted_brand', 'extracted_price', 'extracted_weight', 'extracted_category']\n",
    "        coverage_data = []\n",
    "        for field in fields:\n",
    "            if field in success_df.columns:\n",
    "                populated = success_df[field].notna().sum()\n",
    "                coverage_data.append({\n",
    "                    \"Field\": field.replace('extracted_', ''),\n",
    "                    \"Populated\": populated,\n",
    "                    \"Coverage\": f\"{populated}/{success_count}\",\n",
    "                    \"Percentage\": f\"{populated/success_count:.0%}\"\n",
    "                })\n",
    "        \n",
    "        coverage_df = pd.DataFrame(coverage_data)\n",
    "        print(f\"\\nField Coverage (of successful extractions):\\n\")\n",
    "        display(coverage_df)\n",
    "        \n",
    "        avg_coverage = coverage_df['Populated'].sum() / (len(coverage_df) * success_count) * 100\n",
    "        print(f\"\\n\u2713 Average field coverage: {avg_coverage:.0f}%\")\n",
    "    else:\n",
    "        print(\"\\nNo successful extractions to evaluate\")\n",
    "else:\n",
    "    print(\"\\nNo data to evaluate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}