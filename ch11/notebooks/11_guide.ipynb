{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Chapter 11 Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_4_1",
   "metadata": {},
   "source": [
    "## 11.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "listing_11_1",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  [1] GR1",
      "       https://www.goruck.com/collections/gr1?srsltid=AfmBOoqgAX5xMipioTMJ2Q2tShBKUmht1LajhCfX0UiEiZNgGS2n6Jkt",
      "  [2] GR1",
      "       https://www.goruck.com/products/gr1-usa?srsltid=AfmBOoowHdfC_HUysWWLQaYEPMbwPPSxpf-VC1fdajBevE14xn5Y61xK",
      "  [3] Goruck Gr1 26 liter worth it? : r/onebag",
      "       https://www.reddit.com/r/onebag/comments/1fydqja/goruck_gr1_26_liter_worth_it/",
      ""
     ]
    }
   ],
   "source": [
    "import os  #A\n",
    "import requests  #B\n",
    "\n",
    "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")  #C\n",
    "\n",
    "def search_product_urls(search_key: str, num_results: int = 5) -> list[dict]:  #D\n",
    "    \"\"\"Search for product page candidates using SerpAPI.\"\"\"\n",
    "    params = {  #E\n",
    "        \"q\": search_key,\n",
    "        \"api_key\": SERPAPI_KEY,\n",
    "        \"num\": num_results,\n",
    "        \"engine\": \"google\",\n",
    "    }\n",
    "    resp = requests.get(\"https://serpapi.com/search\", params=params)  #F\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    \n",
    "    candidates = []  #G\n",
    "    for result in data.get(\"organic_results\", []):\n",
    "        candidates.append({\n",
    "            \"title\": result.get(\"title\", \"\"),\n",
    "            \"url\": result.get(\"link\", \"\"),\n",
    "            \"snippet\": result.get(\"snippet\", \"\"),\n",
    "            \"position\": result.get(\"position\", 0),\n",
    "        })\n",
    "    return candidates  #H\n",
    "\n",
    "# Example usage\n",
    "candidates = search_product_urls(\"GORUCK GR1 26L\")  #I\n",
    "for c in candidates[:3]:  # Show first 3 to save space\n",
    "    print(f\"  [{c['position']}] {c['title']}\")\n",
    "    print(f\"       {c['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_4_2",
   "metadata": {},
   "source": [
    "## 11.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "listing_11_2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best URL: https://www.goruck.com/products/gr1-usa?srsltid=AfmBOoowHdfC_HUysWWLQaYEPMbwPPSxpf-VC1fdajBevE14xn5Y61xK",
      "Confidence: High",
      "Reasoning: The second URL (https://www.goruck.com/products/gr1-usa) is the best choice because it appears to be a direct link to the individual product page on the official GORUCK website. This page likely contains detailed product information, including the name, description, price, images, and specifications like weight, making it ideal for extracting structured product data. The first URL seems more like a category or collection page, which might list multiple products. The third URL is a Reddit discussion, which is unsuitable for structured product data extraction.",
      ""
     ]
    }
   ],
   "source": [
    "import openai  #A\n",
    "from pydantic import BaseModel  #B\n",
    "\n",
    "class URLRanking(BaseModel):  #C\n",
    "    best_url: str\n",
    "    confidence: str  # \"high\", \"medium\", \"low\"\n",
    "    reasoning: str\n",
    "\n",
    "def rank_urls_with_ai(  #D\n",
    "    search_key: str,\n",
    "    candidates: list[dict],\n",
    "    model: str = \"gpt-4o\",\n",
    ") -> URLRanking:\n",
    "    \"\"\"Use an LLM to pick the best product page from search results.\"\"\"\n",
    "    \n",
    "    candidate_text = \"\"  #E\n",
    "    for c in candidates[:3]:  # Limit to save tokens\n",
    "        candidate_text += (\n",
    "            f\"Position {c['position']}:\\n\"\n",
    "            f\"  Title: {c['title']}\\n\"\n",
    "            f\"  URL: {c['url']}\\n\"\n",
    "            f\"  Snippet: {c['snippet']}\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    system_prompt = \"\"\"You are a data engineering assistant helping build a product database.\n",
    "Given a product search key and a list of candidate URLs from search results,\n",
    "pick the single best URL for extracting structured product data.\n",
    "\n",
    "Prefer:\n",
    "1. Manufacturer or official brand pages\n",
    "2. Pages likely to contain: product name, price, description, weight, images\n",
    "3. Individual product pages over category or listing pages\n",
    "4. Stable URLs over session-specific or filtered URLs\n",
    "\n",
    "Avoid:\n",
    "- Review sites, forums, Reddit threads\n",
    "- Retailer pages when a manufacturer page is available\n",
    "- Category pages that list multiple products\n",
    "\n",
    "Return the best URL, your confidence level, and a brief explanation.\"\"\"  #F\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Product: {search_key}\\n\\nCandidate URLs:\\n{candidate_text}\"\n",
    "    )  #G\n",
    "    \n",
    "    response = openai.beta.chat.completions.parse(  #H\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        response_format=URLRanking,  #I\n",
    "    )\n",
    "    return response.choices[0].message.parsed  #J\n",
    "\n",
    "# Example usage\n",
    "ranking = rank_urls_with_ai(\"GORUCK GR1 26L\", candidates)  #K\n",
    "print(f\"Best URL: {ranking.best_url}\")\n",
    "print(f\"Confidence: {ranking.confidence}\")\n",
    "print(f\"Reasoning: {ranking.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_5_1",
   "metadata": {},
   "source": [
    "## 11.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "listing_11_3",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Raw HTML: 2,032,625 characters",
      "Cleaned:  20,419 characters",
      "Reduction: 99%",
      ""
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  #A\n",
    "\n",
    "REMOVE_TAGS = [  #B\n",
    "    \"script\", \"style\", \"nav\", \"footer\", \"header\",\n",
    "    \"iframe\", \"noscript\", \"svg\", \"form\",\n",
    "]\n",
    "\n",
    "REMOVE_CLASSES = [  #C\n",
    "    \"breadcrumb\", \"related-products\", \"recently-viewed\",\n",
    "    \"newsletter\", \"cookie-banner\", \"site-footer\",\n",
    "    \"site-header\", \"cart-drawer\", \"search-modal\",\n",
    "    \"review\", \"reviews\", \"ratings\",\n",
    "]\n",
    "\n",
    "def clean_html_aggressive(html: str) -> str:  #D\n",
    "    \"\"\"Remove non-product HTML elements to reduce noise and token count.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted tags entirely\n",
    "    for tag_name in REMOVE_TAGS:  #E\n",
    "        for element in soup.find_all(tag_name):\n",
    "            element.decompose()\n",
    "    \n",
    "    # Remove elements by class name patterns\n",
    "    for class_pattern in REMOVE_CLASSES:  #F\n",
    "        for element in soup.find_all(\n",
    "            class_=lambda c: c and class_pattern in \" \".join(c).lower()\n",
    "        ):\n",
    "            element.decompose()\n",
    "    \n",
    "    # Remove empty elements\n",
    "    for element in soup.find_all():  #G\n",
    "        if not element.get_text(strip=True) and not element.find(\"img\"):\n",
    "            element.decompose()\n",
    "    \n",
    "    clean_text = \" \".join(soup.stripped_strings)  #H\n",
    "    return clean_text\n",
    "\n",
    "# Example usage\n",
    "raw_html = requests.get(\"https://www.goruck.com/products/gr1\").text  #I\n",
    "clean = clean_html_aggressive(raw_html)\n",
    "print(f\"Raw HTML: {len(raw_html):,} characters\")\n",
    "print(f\"Cleaned:  {len(clean):,} characters\")\n",
    "print(f\"Reduction: {(1 - len(clean) / len(raw_html)) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_5_2",
   "metadata": {},
   "source": [
    "## 11.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "listing_11_4",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Product sections: []",
      "Non-product sections: [0, 1, 2, 3, 4]",
      ""
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel  #A\n",
    "\n",
    "class ContentTriage(BaseModel):  #B\n",
    "    product_sections: list[int]\n",
    "    non_product_sections: list[int]\n",
    "\n",
    "def triage_content(text_blocks: list[str], model: str = \"gpt-4o-mini\") -> ContentTriage:  #C\n",
    "    \"\"\"Use a lightweight LLM to classify text blocks as product or non-product.\"\"\"\n",
    "    blocks_text = \"\"  #D\n",
    "    for i, block in enumerate(text_blocks[:5]):  # Limit for demo\n",
    "        blocks_text += f\"[Block {i}]: {block[:100]}...\\n\\n\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a data engineering assistant. Given a list of text blocks\n",
    "from a product web page, classify each block as either product-relevant or not.\n",
    "\n",
    "Product-relevant blocks contain: product name, price, description, specifications,\n",
    "weight, dimensions, materials, features, or sizing information.\n",
    "\n",
    "Non-product blocks contain: navigation, shipping info, return policies, reviews,\n",
    "promotional banners, newsletter signups, or generic site content.\n",
    "\n",
    "Return two lists: product_sections (the block numbers that contain product data)\n",
    "and non_product_sections (everything else).\"\"\"  #E\n",
    "    \n",
    "    response = openai.beta.chat.completions.parse(  #F\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": blocks_text},\n",
    "        ],\n",
    "        response_format=ContentTriage,\n",
    "    )\n",
    "    return response.choices[0].message.parsed  #G\n",
    "\n",
    "# Example - split cleaned text into blocks\n",
    "blocks = clean.split('.')[: 10]  # First 10 sentences\n",
    "triage = triage_content(blocks)\n",
    "print(f\"Product sections: {triage.product_sections}\")\n",
    "print(f\"Non-product sections: {triage.non_product_sections}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_6_1",
   "metadata": {},
   "source": [
    "## 11.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "listing_11_5",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ProductExtraction schema defined",
      ""
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field  #A\n",
    "from typing import Optional  #B\n",
    "\n",
    "class ProductExtraction(BaseModel):  #C\n",
    "    \"\"\"Schema for extracting product data from web page content.\"\"\"\n",
    "    product_name: str = Field(  #D\n",
    "        description=\"Full product name as shown on the page\"\n",
    "    )\n",
    "    brand_name: str = Field(  #E\n",
    "        description=\"Manufacturer or brand name\"\n",
    "    )\n",
    "    description: Optional[str] = Field(  #F\n",
    "        default=None,\n",
    "        description=\"Product description, typically 1-3 sentences\"\n",
    "    )\n",
    "    price: Optional[str] = Field(  #G\n",
    "        default=None,\n",
    "        description=\"Current retail price including currency symbol\"\n",
    "    )\n",
    "    weight: Optional[str] = Field(  #H\n",
    "        default=None,\n",
    "        description=\"Product weight with unit (e.g., '2.5 lbs', '1.1 kg')\"\n",
    "    )\n",
    "    primary_image_url: Optional[str] = Field(  #I\n",
    "        default=None,\n",
    "        description=\"URL of the main product image\"\n",
    "    )\n",
    "    category: Optional[str] = Field(  #J\n",
    "        default=None,\n",
    "        description=\"Product category (e.g., backpack, tent, sleeping bag)\"\n",
    "    )\n",
    "\n",
    "# Schema is now defined and ready to use\n",
    "print(\"ProductExtraction schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_6_2",
   "metadata": {},
   "source": [
    "## 11.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "listing_11_6",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Name:        GR1",
      "Brand:       GORUCK",
      "Price:       None",
      "Weight:      None",
      "Category:    backpack",
      ""
     ]
    }
   ],
   "source": [
    "import openai  #A\n",
    "\n",
    "EXTRACTION_PROMPT = \"\"\"You are a product data extraction assistant for a data engineering pipeline.\n",
    "\n",
    "Given the text content of a product web page, extract the following fields accurately:\n",
    "- product_name: The full product name as displayed on the page\n",
    "- brand_name: The manufacturer or brand\n",
    "- description: A concise product description (1-3 sentences)\n",
    "- price: The current retail price with currency symbol\n",
    "- weight: The product weight with unit if available\n",
    "- primary_image_url: The URL of the main product image if found in the text\n",
    "- category: The product category (backpack, tent, sleeping bag, headlamp, etc.)\n",
    "\n",
    "Rules:\n",
    "- Only extract information that is explicitly present in the text\n",
    "- Use null for any field you cannot find or confidently determine\n",
    "- Do not guess or fabricate values\n",
    "- For price, use the current or sale price, not the original price if both are shown\n",
    "- For weight, include the unit (lbs, oz, kg, g)\n",
    "- For category, use a simple label based on what the product is\"\"\"  #B\n",
    "\n",
    "def extract_product_with_ai(  #C\n",
    "    cleaned_text: str,\n",
    "    model: str = \"gpt-4o\",\n",
    ") -> ProductExtraction:\n",
    "    \"\"\"Extract product fields from cleaned page text using an LLM.\"\"\"\n",
    "    response = openai.beta.chat.completions.parse(  #D\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EXTRACTION_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": cleaned_text[:3000]},  #E - Limit for demo\n",
    "        ],\n",
    "        response_format=ProductExtraction,  #F\n",
    "    )\n",
    "    return response.choices[0].message.parsed  #G\n",
    "\n",
    "# Example usage\n",
    "product = extract_product_with_ai(clean)  #H\n",
    "\n",
    "print(f\"Name:        {product.product_name}\")\n",
    "print(f\"Brand:       {product.brand_name}\")\n",
    "print(f\"Price:       {product.price}\")\n",
    "print(f\"Weight:      {product.weight}\")\n",
    "print(f\"Category:    {product.category}\")\n",
    "if product.description:\n",
    "    print(f\"Description: {product.description[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_6_3",
   "metadata": {},
   "source": [
    "## 11.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "listing_11_7",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       Field            Manual     AI",
      "product_name GR1 USA - Cordura    GR1",
      "  brand_name            GORUCK GORUCK",
      "       price           $335.00    NaN",
      "      weight               NaN    NaN",
      "    category               NaN    NaN",
      " description               NaN    NaN",
      ""
     ]
    }
   ],
   "source": [
    "import pandas as pd  #A\n",
    "from bs4 import BeautifulSoup  #B\n",
    "\n",
    "def extract_manual_goruck(html: str) -> dict:  #C\n",
    "    \"\"\"Manual extraction using Chapter 10's CSS selector approach.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    title_el = soup.find(\"h1\")\n",
    "    title = title_el.get_text(\" \", strip=True) if title_el else None\n",
    "    \n",
    "    price_el = (\n",
    "        soup.select_one(\n",
    "            \"div.product-block__price span.price-item--sale.price-item--last\"\n",
    "        )\n",
    "        or soup.select_one(\"div.product-block__price span.price-item--regular\")\n",
    "    )\n",
    "    price = price_el.get_text(\" \", strip=True) if price_el else None\n",
    "    \n",
    "    return {\n",
    "        \"product_name\": title,\n",
    "        \"brand_name\": \"GORUCK\",  # hardcoded for this site\n",
    "        \"price\": price,\n",
    "        \"weight\": None,  # manual approach did not extract this\n",
    "        \"category\": None,  # manual approach did not extract this\n",
    "        \"description\": None,  # manual approach did not extract this\n",
    "    }\n",
    "\n",
    "# Run both approaches on the same page\n",
    "url = \"https://www.goruck.com/products/gr1\"  #D\n",
    "raw_html = requests.get(url).text\n",
    "manual_result = extract_manual_goruck(raw_html)  #E\n",
    "\n",
    "cleaned = clean_html_aggressive(raw_html)\n",
    "ai_result = extract_product_with_ai(cleaned)  #F\n",
    "\n",
    "# Build comparison table\n",
    "comparison = pd.DataFrame({  #G\n",
    "    \"Field\": [\"product_name\", \"brand_name\", \"price\", \"weight\",\n",
    "              \"category\", \"description\"],\n",
    "    \"Manual\": [\n",
    "        manual_result[\"product_name\"],\n",
    "        manual_result[\"brand_name\"],\n",
    "        manual_result[\"price\"],\n",
    "        manual_result[\"weight\"],\n",
    "        manual_result[\"category\"],\n",
    "        manual_result[\"description\"],\n",
    "    ],\n",
    "    \"AI\": [\n",
    "        ai_result.product_name,\n",
    "        ai_result.brand_name,\n",
    "        ai_result.price,\n",
    "        ai_result.weight,\n",
    "        ai_result.category,\n",
    "        ai_result.description[:50] if ai_result.description else None,\n",
    "    ],\n",
    "})\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_7_1",
   "metadata": {},
   "source": [
    "## 11.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "listing_11_8",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    search_key  status product_name price weight category",
      "GORUCK GR1 26L success          GR1  None   None     None",
      ""
     ]
    }
   ],
   "source": [
    "import time  #A\n",
    "import pandas as pd  #B\n",
    "\n",
    "test_products = [  #D\n",
    "    {\"name\": \"GORUCK GR1 26L\",\n",
    "     \"url\": \"https://www.goruck.com/products/gr1\"},\n",
    "]\n",
    "\n",
    "results = []  #E\n",
    "for product in test_products:  # Demo with 1 product to save time/cost\n",
    "    record = {  #F\n",
    "        \"search_key\": product[\"name\"],\n",
    "        \"url\": product[\"url\"],\n",
    "        \"status\": \"error\",\n",
    "    }\n",
    "    try:\n",
    "        raw_html = requests.get(product[\"url\"]).text  #G\n",
    "        cleaned = clean_html_aggressive(raw_html)\n",
    "        extraction = extract_product_with_ai(cleaned)  #H\n",
    "        \n",
    "        record[\"product_name\"] = extraction.product_name\n",
    "        record[\"brand_name\"] = extraction.brand_name\n",
    "        record[\"price\"] = extraction.price\n",
    "        record[\"weight\"] = extraction.weight\n",
    "        record[\"category\"] = extraction.category\n",
    "        record[\"description\"] = (\n",
    "            extraction.description[:60] + \"...\"\n",
    "            if extraction.description\n",
    "            else None\n",
    "        )\n",
    "        record[\"status\"] = \"success\"  #I\n",
    "    except Exception as e:\n",
    "        record[\"status\"] = f\"error: {type(e).__name__}\"  #J\n",
    "    \n",
    "    results.append(record)\n",
    "    time.sleep(1)  #K\n",
    "\n",
    "out = pd.DataFrame(results)  #L\n",
    "print(out[[\"search_key\", \"status\", \"product_name\", \"price\", \"weight\", \"category\"]]\n",
    "      .to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_8",
   "metadata": {},
   "source": [
    "## 11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "listing_11_9",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input tokens:  5,121",
      "Output tokens: 300 (estimated)",
      "Cost per page: $0.0158",
      "Cost for 450 products: $7.11",
      ""
     ]
    }
   ],
   "source": [
    "import tiktoken  #A\n",
    "\n",
    "def estimate_extraction_cost(  #B\n",
    "    text: str,\n",
    "    model: str = \"gpt-4o\",\n",
    "    output_tokens: int = 300,\n",
    ") -> dict:\n",
    "    \"\"\"Estimate the token count and cost for an extraction call.\"\"\"\n",
    "    encoder = tiktoken.encoding_for_model(model)  #C\n",
    "    input_tokens = len(encoder.encode(text))  #D\n",
    "    \n",
    "    # Pricing as of mid-2025 (check docs for current rates)\n",
    "    pricing = {  #E\n",
    "        \"gpt-4o\": {\"input\": 2.50 / 1_000_000, \"output\": 10.00 / 1_000_000},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15 / 1_000_000, \"output\": 0.60 / 1_000_000},\n",
    "    }\n",
    "    \n",
    "    rates = pricing.get(model, pricing[\"gpt-4o\"])  #F\n",
    "    input_cost = input_tokens * rates[\"input\"]\n",
    "    output_cost = output_tokens * rates[\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {  #G\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens_est\": output_tokens,\n",
    "        \"input_cost\": round(input_cost, 6),\n",
    "        \"output_cost\": round(output_cost, 6),\n",
    "        \"total_cost\": round(total_cost, 6),\n",
    "    }\n",
    "\n",
    "# Estimate for a single product page\n",
    "cost = estimate_extraction_cost(clean)  #H\n",
    "print(f\"Input tokens:  {cost['input_tokens']:,}\")\n",
    "print(f\"Output tokens: {cost['output_tokens_est']:,} (estimated)\")\n",
    "print(f\"Cost per page: ${cost['total_cost']:.4f}\")\n",
    "print(f\"Cost for 450 products: ${cost['total_cost'] * 450:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}