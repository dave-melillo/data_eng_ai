{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Chapter 11 Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "traceback": [
      "No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"\u2713 Environment variables loaded\")\n",
    "print(\"Ready to run listings 11.1-11.3, 11.5-11.9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_4_1",
   "metadata": {},
   "source": [
    "## 11.4.1 - Listing 11.1: Search Product URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "listing_11_1",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================",
      "LISTING 11.1: Search Product URLs with SerpAPI",
      "============================================================",
      "",
      "Searching for: 'GORUCK GR1 26L'",
      "",
      "Found 3 candidate URLs:",
      "",
      "   position  ...                                                url",
      "0         1  ...  https://www.goruck.com/collections/gr1?srsltid...",
      "1         2  ...  https://www.goruck.com/products/gr1-usa?srslti...",
      "2         3  ...  https://www.reddit.com/r/onebag/comments/1fydq...",
      "",
      "[3 rows x 3 columns]",
      "",
      "\u2713 3 URLs ready for AI ranking",
      ""
     ]
    }
   ],
   "source": [
    "import os  #A\n",
    "import requests  #B\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.1: Search Product URLs with SerpAPI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")  #C\n",
    "\n",
    "def search_product_urls(search_key: str, num_results: int = 5) -> list[dict]:  #D\n",
    "    \"\"\"Search for product page candidates using SerpAPI.\"\"\"\n",
    "    params = {  #E\n",
    "        \"q\": search_key,\n",
    "        \"api_key\": SERPAPI_KEY,\n",
    "        \"num\": num_results,\n",
    "        \"engine\": \"google\",\n",
    "    }\n",
    "    resp = requests.get(\"https://serpapi.com/search\", params=params)  #F\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    \n",
    "    candidates = []  #G\n",
    "    for result in data.get(\"organic_results\", []):\n",
    "        candidates.append({\n",
    "            \"title\": result.get(\"title\", \"\"),\n",
    "            \"url\": result.get(\"link\", \"\"),\n",
    "            \"snippet\": result.get(\"snippet\", \"\"),\n",
    "            \"position\": result.get(\"position\", 0),\n",
    "        })\n",
    "    return candidates  #H\n",
    "\n",
    "# Example usage\n",
    "search_query = \"GORUCK GR1 26L\"\n",
    "print(f\"\\nSearching for: '{search_query}'\\n\")\n",
    "\n",
    "candidates = search_product_urls(search_query)  #I\n",
    "\n",
    "# Display as DataFrame\n",
    "df_candidates = pd.DataFrame(candidates)\n",
    "print(f\"Found {len(candidates)} candidate URLs:\\n\")\n",
    "display(df_candidates[['position', 'title', 'url']])\n",
    "\n",
    "print(f\"\\n\u2713 {len(candidates)} URLs ready for AI ranking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_4_2",
   "metadata": {},
   "source": [
    "## 11.4.2 - Listing 11.2: Rank URLs with AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "listing_11_2",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================",
      "LISTING 11.2: Rank URLs with AI",
      "============================================================",
      "",
      "Asking AI to rank 3 URLs...",
      "",
      "Best URL (Confidence: High):",
      "https://www.goruck.com/products/gr1-usa?srsltid=AfmBOoowHdfC_HUysWWLQaYEPMbwPPSxpf-VC1fdajBevE14xn5Y61xK",
      "",
      "Reasoning:",
      "The second URL 'https://www.goruck.com/products/gr1-usa?srsltid=AfmBOoowHdfC_HUysWWLQaYEPMbwPPSxpf-VC1fdajBevE14xn5Y61xK' appears to be a direct product page, which typically includes specific details like product name, price, description, specifications, and images. This URL is more likely to lead to an individual product page compared to the collection page in the first URL. Also, it belongs to the official GORUCK website, making it authoritative and likely to have the most accurate and comprehensive product information.",
      "",
      "\u2713 Best URL selected for extraction",
      ""
     ]
    }
   ],
   "source": [
    "import openai  #A\n",
    "from pydantic import BaseModel  #B\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.2: Rank URLs with AI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class URLRanking(BaseModel):  #C\n",
    "    best_url: str\n",
    "    confidence: str  # \"high\", \"medium\", \"low\"\n",
    "    reasoning: str\n",
    "\n",
    "def rank_urls_with_ai(  #D\n",
    "    search_key: str,\n",
    "    candidates: list[dict],\n",
    "    model: str = \"gpt-4o\",\n",
    ") -> URLRanking:\n",
    "    \"\"\"Use an LLM to pick the best product page from search results.\"\"\"\n",
    "    \n",
    "    candidate_text = \"\"  #E\n",
    "    for c in candidates:\n",
    "        candidate_text += (\n",
    "            f\"Position {c['position']}:\\n\"\n",
    "            f\"  Title: {c['title']}\\n\"\n",
    "            f\"  URL: {c['url']}\\n\"\n",
    "            f\"  Snippet: {c['snippet']}\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    system_prompt = \"\"\"You are a data engineering assistant helping build a product database.\n",
    "Given a product search key and a list of candidate URLs from search results,\n",
    "pick the single best URL for extracting structured product data.\n",
    "\n",
    "Prefer:\n",
    "1. Manufacturer or official brand pages\n",
    "2. Pages likely to contain: product name, price, description, weight, images\n",
    "3. Individual product pages over category or listing pages\n",
    "4. Stable URLs over session-specific or filtered URLs\n",
    "\n",
    "Avoid:\n",
    "- Review sites, forums, Reddit threads\n",
    "- Retailer pages when a manufacturer page is available\n",
    "- Category pages that list multiple products\n",
    "\n",
    "Return the best URL, your confidence level, and a brief explanation.\"\"\"  #F\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Product: {search_key}\\n\\nCandidate URLs:\\n{candidate_text}\"\n",
    "    )  #G\n",
    "    \n",
    "    response = openai.beta.chat.completions.parse(  #H\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        response_format=URLRanking,  #I\n",
    "    )\n",
    "    return response.choices[0].message.parsed  #J\n",
    "\n",
    "# Example usage\n",
    "print(f\"\\nAsking AI to rank {len(candidates)} URLs...\\n\")\n",
    "ranking = rank_urls_with_ai(search_query, candidates)  #K\n",
    "\n",
    "# Display results\n",
    "print(f\"Best URL (Confidence: {ranking.confidence}):\")\n",
    "print(f\"{ranking.best_url}\\n\")\n",
    "print(f\"Reasoning:\")\n",
    "print(f\"{ranking.reasoning}\\n\")\n",
    "print(f\"\u2713 Best URL selected for extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_5_1",
   "metadata": {},
   "source": [
    "## 11.5.1 - Listing 11.3: Aggressive HTML Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "listing_11_3",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================",
      "LISTING 11.3: Aggressive HTML Cleaning",
      "============================================================",
      "",
      "Fetching HTML from: https://www.goruck.com/products/gr1-usa?srsltid=AfmBOoowHdfC_HUysWWLQaYEPMbwPPSxpf-VC1fdajBevE14xn5Y61xK",
      "",
      "Cleaning Results:",
      "  Raw HTML:    2,032,630 characters",
      "  Cleaned:     20,419 characters",
      "  Reduction:   99.0%",
      "",
      "First 200 characters of cleaned text:",
      "GR1 | GORUCK Skip to content Presidents Day Sale | Steals, Deals & Bundles FREE RUCK PLATES WITH BASIC RUCKER FREE CURVED PLATES WITH RUCKING WEIGHT VEST FREE USA Shipping for GORUCK Tribe Members Pro...",
      "",
      "\u2713 HTML cleaned and ready for extraction",
      ""
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  #A\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.3: Aggressive HTML Cleaning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "REMOVE_TAGS = [  #B\n",
    "    \"script\", \"style\", \"nav\", \"footer\", \"header\",\n",
    "    \"iframe\", \"noscript\", \"svg\", \"form\",\n",
    "]\n",
    "\n",
    "REMOVE_CLASSES = [  #C\n",
    "    \"breadcrumb\", \"related-products\", \"recently-viewed\",\n",
    "    \"newsletter\", \"cookie-banner\", \"site-footer\",\n",
    "    \"site-header\", \"cart-drawer\", \"search-modal\",\n",
    "    \"review\", \"reviews\", \"ratings\",\n",
    "]\n",
    "\n",
    "def clean_html_aggressive(html: str) -> str:  #D\n",
    "    \"\"\"Remove non-product HTML elements to reduce noise and token count.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted tags entirely\n",
    "    for tag_name in REMOVE_TAGS:  #E\n",
    "        for element in soup.find_all(tag_name):\n",
    "            element.decompose()\n",
    "    \n",
    "    # Remove elements by class name patterns\n",
    "    for class_pattern in REMOVE_CLASSES:  #F\n",
    "        for element in soup.find_all(\n",
    "            class_=lambda c: c and class_pattern in \" \".join(c).lower()\n",
    "        ):\n",
    "            element.decompose()\n",
    "    \n",
    "    # Remove empty elements\n",
    "    for element in soup.find_all():  #G\n",
    "        if not element.get_text(strip=True) and not element.find(\"img\"):\n",
    "            element.decompose()\n",
    "    \n",
    "    clean_text = \" \".join(soup.stripped_strings)  #H\n",
    "    return clean_text\n",
    "\n",
    "# Example usage - fetch the best URL from previous step\n",
    "print(f\"\\nFetching HTML from: {ranking.best_url}\\n\")\n",
    "raw_html = requests.get(ranking.best_url).text  #I\n",
    "clean = clean_html_aggressive(raw_html)\n",
    "\n",
    "# Display cleaning stats\n",
    "reduction_pct = (1 - len(clean) / len(raw_html)) * 100\n",
    "print(f\"Cleaning Results:\")\n",
    "print(f\"  Raw HTML:    {len(raw_html):,} characters\")\n",
    "print(f\"  Cleaned:     {len(clean):,} characters\")\n",
    "print(f\"  Reduction:   {reduction_pct:.1f}%\\n\")\n",
    "print(f\"First 200 characters of cleaned text:\")\n",
    "print(f\"{clean[:200]}...\\n\")\n",
    "print(f\"\u2713 HTML cleaned and ready for extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_6_1",
   "metadata": {},
   "source": [
    "## 11.6.1 - Listing 11.5: Define Extraction Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "listing_11_5",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================",
      "LISTING 11.5: Define Extraction Schema",
      "============================================================",
      "",
      "ProductExtraction Schema:",
      "  \u2022 product_name         (required)",
      "  \u2022 brand_name           (required)",
      "  \u2022 description          (optional)",
      "  \u2022 price                (optional)",
      "  \u2022 weight               (optional)",
      "  \u2022 primary_image_url    (optional)",
      "  \u2022 category             (optional)",
      "",
      "\u2713 Schema defined with 7 fields",
      ""
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field  #A\n",
    "from typing import Optional  #B\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.5: Define Extraction Schema\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class ProductExtraction(BaseModel):  #C\n",
    "    \"\"\"Schema for extracting product data from web page content.\"\"\"\n",
    "    product_name: str = Field(  #D\n",
    "        description=\"Full product name as shown on the page\"\n",
    "    )\n",
    "    brand_name: str = Field(  #E\n",
    "        description=\"Manufacturer or brand name\"\n",
    "    )\n",
    "    description: Optional[str] = Field(  #F\n",
    "        default=None,\n",
    "        description=\"Product description, typically 1-3 sentences\"\n",
    "    )\n",
    "    price: Optional[str] = Field(  #G\n",
    "        default=None,\n",
    "        description=\"Current retail price including currency symbol\"\n",
    "    )\n",
    "    weight: Optional[str] = Field(  #H\n",
    "        default=None,\n",
    "        description=\"Product weight with unit (e.g., '2.5 lbs', '1.1 kg')\"\n",
    "    )\n",
    "    primary_image_url: Optional[str] = Field(  #I\n",
    "        default=None,\n",
    "        description=\"URL of the main product image\"\n",
    "    )\n",
    "    category: Optional[str] = Field(  #J\n",
    "        default=None,\n",
    "        description=\"Product category (e.g., backpack, tent, sleeping bag)\"\n",
    "    )\n",
    "\n",
    "# Display schema\n",
    "print(\"\\nProductExtraction Schema:\")\n",
    "for field_name, field_info in ProductExtraction.model_fields.items():\n",
    "    required = \"required\" if field_info.is_required() else \"optional\"\n",
    "    print(f\"  \u2022 {field_name:20s} ({required})\")\n",
    "\n",
    "print(f\"\\n\u2713 Schema defined with {len(ProductExtraction.model_fields)} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_6_2",
   "metadata": {},
   "source": [
    "## 11.6.2 - Listing 11.6: AI Product Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "listing_11_6",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================",
      "LISTING 11.6: AI Product Extraction",
      "============================================================",
      "",
      "Extracting product data from cleaned HTML...",
      "",
      "Extracted Product Data:",
      "  Name:        GR1",
      "  Brand:       GORUCK",
      "  Price:       $335.00",
      "  Weight:      2.8 LBS",
      "  Category:    backpack",
      "  Description: The GR1 backpack is designed to withstand extreme conditions and has been tested by Green Berets to ...",
      "",
      "\u2713 Product data extracted successfully",
      ""
     ]
    }
   ],
   "source": [
    "import openai  #A\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.6: AI Product Extraction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "EXTRACTION_PROMPT = \"\"\"You are a product data extraction assistant for a data engineering pipeline.\n",
    "\n",
    "Given the text content of a product web page, extract the following fields accurately:\n",
    "- product_name: The full product name as displayed on the page\n",
    "- brand_name: The manufacturer or brand\n",
    "- description: A concise product description (1-3 sentences)\n",
    "- price: The current retail price with currency symbol\n",
    "- weight: The product weight with unit if available\n",
    "- primary_image_url: The URL of the main product image if found in the text\n",
    "- category: The product category (backpack, tent, sleeping bag, headlamp, etc.)\n",
    "\n",
    "Rules:\n",
    "- Only extract information that is explicitly present in the text\n",
    "- Use null for any field you cannot find or confidently determine\n",
    "- Do not guess or fabricate values\n",
    "- For price, use the current or sale price, not the original price if both are shown\n",
    "- For weight, include the unit (lbs, oz, kg, g)\n",
    "- For category, use a simple label based on what the product is\"\"\"  #B\n",
    "\n",
    "def extract_product_with_ai(  #C\n",
    "    cleaned_text: str,\n",
    "    model: str = \"gpt-4o\",\n",
    ") -> ProductExtraction:\n",
    "    \"\"\"Extract product fields from cleaned page text using an LLM.\"\"\"\n",
    "    response = openai.beta.chat.completions.parse(  #D\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EXTRACTION_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": cleaned_text[:8000]},  #E\n",
    "        ],\n",
    "        response_format=ProductExtraction,  #F\n",
    "    )\n",
    "    return response.choices[0].message.parsed  #G\n",
    "\n",
    "# Example usage\n",
    "print(f\"\\nExtracting product data from cleaned HTML...\\n\")\n",
    "product = extract_product_with_ai(clean)  #H\n",
    "\n",
    "# Display extraction results\n",
    "print(f\"Extracted Product Data:\")\n",
    "print(f\"  Name:        {product.product_name}\")\n",
    "print(f\"  Brand:       {product.brand_name}\")\n",
    "print(f\"  Price:       {product.price}\")\n",
    "print(f\"  Weight:      {product.weight}\")\n",
    "print(f\"  Category:    {product.category}\")\n",
    "if product.description:\n",
    "    desc_preview = product.description[:100] + \"...\" if len(product.description) > 100 else product.description\n",
    "    print(f\"  Description: {desc_preview}\")\n",
    "\n",
    "print(f\"\\n\u2713 Product data extracted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_6_3",
   "metadata": {},
   "source": [
    "## 11.6.3 - Listing 11.7: Manual vs AI Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "listing_11_7",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================",
      "LISTING 11.7: Manual vs AI Extraction Comparison",
      "============================================================",
      "",
      "Comparing manual (CSS selectors) vs AI extraction...",
      "",
      "          Field  ...                                         AI (Ch 11)",
      "0  product_name  ...                                                GR1",
      "1    brand_name  ...                                             GORUCK",
      "2         price  ...                                            $335.00",
      "3        weight  ...                                            2.8 LBS",
      "4      category  ...                                           backpack",
      "5   description  ...  The GR1 backpack is designed to withstand extr...",
      "",
      "[6 rows x 3 columns]",
      "",
      "Comparison Summary:",
      "  Manual approach: 3/6 fields populated",
      "  AI approach:     6/6 fields populated",
      "",
      "\u2713 AI extracted 3 additional fields",
      ""
     ]
    }
   ],
   "source": [
    "import pandas as pd  #A\n",
    "from bs4 import BeautifulSoup  #B\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.7: Manual vs AI Extraction Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def extract_manual_goruck(html: str) -> dict:  #C\n",
    "    \"\"\"Manual extraction using Chapter 10's CSS selector approach.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    title_el = soup.find(\"h1\")\n",
    "    title = title_el.get_text(\" \", strip=True) if title_el else None\n",
    "    \n",
    "    price_el = (\n",
    "        soup.select_one(\n",
    "            \"div.product-block__price span.price-item--sale.price-item--last\"\n",
    "        )\n",
    "        or soup.select_one(\"div.product-block__price span.price-item--regular\")\n",
    "    )\n",
    "    price = price_el.get_text(\" \", strip=True) if price_el else None\n",
    "    \n",
    "    return {\n",
    "        \"product_name\": title,\n",
    "        \"brand_name\": \"GORUCK\",  # hardcoded for this site\n",
    "        \"price\": price,\n",
    "        \"weight\": None,  # manual approach did not extract this\n",
    "        \"category\": None,  # manual approach did not extract this\n",
    "        \"description\": None,  # manual approach did not extract this\n",
    "    }\n",
    "\n",
    "# Run both approaches on the same page\n",
    "print(\"\\nComparing manual (CSS selectors) vs AI extraction...\\n\")\n",
    "manual_result = extract_manual_goruck(raw_html)  #E\n",
    "ai_result = product  #F (from previous cell)\n",
    "\n",
    "# Build comparison table\n",
    "comparison = pd.DataFrame({  #G\n",
    "    \"Field\": [\"product_name\", \"brand_name\", \"price\", \"weight\",\n",
    "              \"category\", \"description\"],\n",
    "    \"Manual (Ch 10)\": [\n",
    "        manual_result[\"product_name\"],\n",
    "        manual_result[\"brand_name\"],\n",
    "        manual_result[\"price\"],\n",
    "        manual_result[\"weight\"],\n",
    "        manual_result[\"category\"],\n",
    "        manual_result[\"description\"],\n",
    "    ],\n",
    "    \"AI (Ch 11)\": [\n",
    "        ai_result.product_name,\n",
    "        ai_result.brand_name,\n",
    "        ai_result.price,\n",
    "        ai_result.weight,\n",
    "        ai_result.category,\n",
    "        ai_result.description[:50] + \"...\" if ai_result.description else None,\n",
    "    ],\n",
    "})\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# Count populated fields\n",
    "manual_populated = sum(1 for v in manual_result.values() if v is not None)\n",
    "ai_populated = sum(1 for v in [ai_result.product_name, ai_result.brand_name, ai_result.price, \n",
    "                                 ai_result.weight, ai_result.category, ai_result.description] if v is not None)\n",
    "\n",
    "print(f\"\\nComparison Summary:\")\n",
    "print(f\"  Manual approach: {manual_populated}/6 fields populated\")\n",
    "print(f\"  AI approach:     {ai_populated}/6 fields populated\")\n",
    "print(f\"\\n\u2713 AI extracted {ai_populated - manual_populated} additional fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_7_1",
   "metadata": {},
   "source": [
    "## 11.7.1 - Listing 11.8: Multi-Site Batch Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "listing_11_8",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================",
      "LISTING 11.8: Multi-Site Batch Extraction",
      "============================================================",
      "",
      "Processing 2 products across multiple sites...",
      "",
      "  \u2022 GORUCK GR1 26L...",
      "  \u2022 Osprey Atmos AG 65...",
      "",
      "Batch Extraction Results:",
      "",
      "           search_key   status       product_name    price   weight  category",
      "0      GORUCK GR1 26L  success  GR1 USA - Cordura  $335.00  2.8 LBS  backpack",
      "1  Osprey Atmos AG 65  success                         NaN      NaN       NaN",
      "",
      "\u2713 2/2 products extracted successfully",
      ""
     ]
    }
   ],
   "source": [
    "import time  #A\n",
    "import pandas as pd  #B\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.8: Multi-Site Batch Extraction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_products = [  #D\n",
    "    {\"name\": \"GORUCK GR1 26L\",\n",
    "     \"url\": \"https://www.goruck.com/products/gr1\"},\n",
    "    {\"name\": \"Osprey Atmos AG 65\",\n",
    "     \"url\": \"https://www.osprey.com/us/en/product/atmos-ag-65-ATMOS65S23.html\"},\n",
    "]\n",
    "\n",
    "print(f\"\\nProcessing {len(test_products)} products across multiple sites...\\n\")\n",
    "\n",
    "results = []  #E\n",
    "for product in test_products:\n",
    "    record = {  #F\n",
    "        \"search_key\": product[\"name\"],\n",
    "        \"url\": product[\"url\"],\n",
    "        \"status\": \"error\",\n",
    "    }\n",
    "    try:\n",
    "        print(f\"  \u2022 {product['name']}...\")\n",
    "        raw_html = requests.get(product[\"url\"]).text  #G\n",
    "        cleaned = clean_html_aggressive(raw_html)\n",
    "        extraction = extract_product_with_ai(cleaned)  #H\n",
    "        \n",
    "        record[\"product_name\"] = extraction.product_name\n",
    "        record[\"brand_name\"] = extraction.brand_name\n",
    "        record[\"price\"] = extraction.price\n",
    "        record[\"weight\"] = extraction.weight\n",
    "        record[\"category\"] = extraction.category\n",
    "        record[\"description\"] = (\n",
    "            extraction.description[:60] + \"...\"\n",
    "            if extraction.description\n",
    "            else None\n",
    "        )\n",
    "        record[\"status\"] = \"success\"  #I\n",
    "    except Exception as e:\n",
    "        record[\"status\"] = f\"error: {type(e).__name__}\"  #J\n",
    "        print(f\"    Error: {type(e).__name__}\")\n",
    "    \n",
    "    results.append(record)\n",
    "    time.sleep(2)  #K\n",
    "\n",
    "out = pd.DataFrame(results)  #L\n",
    "print(f\"\\nBatch Extraction Results:\\n\")\n",
    "display(out[[\"search_key\", \"status\", \"product_name\", \"price\", \"weight\", \"category\"]])\n",
    "\n",
    "success_count = (out[\"status\"] == \"success\").sum()\n",
    "print(f\"\\n\u2713 {success_count}/{len(test_products)} products extracted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_8",
   "metadata": {},
   "source": [
    "## 11.8 - Listing 11.9: Token & Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "listing_11_9",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================================================",
      "LISTING 11.9: Token & Cost Estimation",
      "============================================================",
      "",
      "Estimating costs for cleaned HTML (model: gpt-4o)...",
      "",
      "Token & Cost Analysis:",
      "  Input tokens:          5,121",
      "  Output tokens (est):   300",
      "  Cost per page:         $0.0158",
      "  Cost for 450 products: $7.11",
      "",
      "\u2713 Cost estimate: ~$7.11 for full pipeline",
      ""
     ]
    }
   ],
   "source": [
    "import tiktoken  #A\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.9: Token & Cost Estimation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def estimate_extraction_cost(  #B\n",
    "    text: str,\n",
    "    model: str = \"gpt-4o\",\n",
    "    output_tokens: int = 300,\n",
    ") -> dict:\n",
    "    \"\"\"Estimate the token count and cost for an extraction call.\"\"\"\n",
    "    encoder = tiktoken.encoding_for_model(model)  #C\n",
    "    input_tokens = len(encoder.encode(text))  #D\n",
    "    \n",
    "    # Pricing as of mid-2025 (check docs for current rates)\n",
    "    pricing = {  #E\n",
    "        \"gpt-4o\": {\"input\": 2.50 / 1_000_000, \"output\": 10.00 / 1_000_000},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15 / 1_000_000, \"output\": 0.60 / 1_000_000},\n",
    "    }\n",
    "    \n",
    "    rates = pricing.get(model, pricing[\"gpt-4o\"])  #F\n",
    "    input_cost = input_tokens * rates[\"input\"]\n",
    "    output_cost = output_tokens * rates[\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {  #G\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens_est\": output_tokens,\n",
    "        \"input_cost\": round(input_cost, 6),\n",
    "        \"output_cost\": round(output_cost, 6),\n",
    "        \"total_cost\": round(total_cost, 6),\n",
    "    }\n",
    "\n",
    "# Estimate for a single product page\n",
    "print(f\"\\nEstimating costs for cleaned HTML (model: gpt-4o)...\\n\")\n",
    "cost = estimate_extraction_cost(clean)  #H\n",
    "\n",
    "print(f\"Token & Cost Analysis:\")\n",
    "print(f\"  Input tokens:          {cost['input_tokens']:,}\")\n",
    "print(f\"  Output tokens (est):   {cost['output_tokens_est']:,}\")\n",
    "print(f\"  Cost per page:         ${cost['total_cost']:.4f}\")\n",
    "print(f\"  Cost for 450 products: ${cost['total_cost'] * 450:.2f}\\n\")\n",
    "print(f\"\u2713 Cost estimate: ~${cost['total_cost'] * 450:.2f} for full pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}