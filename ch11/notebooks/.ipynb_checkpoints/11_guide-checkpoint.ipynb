{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Chapter 11 Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment variables loaded\n",
      "Ready to run listings 11.1-11.9\n"
     ]
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Environment variables loaded\")\n",
    "print(\"Ready to run listings 11.1-11.9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_4_1",
   "metadata": {},
   "source": [
    "## 11.4.1 - Listing 11.1: Search Product URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "listing_11_1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davemelillo/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:61: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISTING 11.1: Search Product URLs with SerpAPI\n",
      "============================================================\n",
      "\n",
      "Searching for: 'GORUCK GR1 26L'\n",
      "\n",
      "Found 3 candidate URLs:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>GR1</td>\n",
       "      <td>https://www.goruck.com/collections/gr1?srsltid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GR1</td>\n",
       "      <td>https://www.goruck.com/products/gr1-usa?srslti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Goruck Gr1 26 liter worth it? : r/onebag</td>\n",
       "      <td>https://www.reddit.com/r/onebag/comments/1fydq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   position                                     title  \\\n",
       "0         1                                       GR1   \n",
       "1         2                                       GR1   \n",
       "2         3  Goruck Gr1 26 liter worth it? : r/onebag   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.goruck.com/collections/gr1?srsltid...  \n",
       "1  https://www.goruck.com/products/gr1-usa?srslti...  \n",
       "2  https://www.reddit.com/r/onebag/comments/1fydq...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 3 URLs ready for AI ranking\n"
     ]
    }
   ],
   "source": [
    "import os  #A\n",
    "import requests  #B\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.1: Search Product URLs with SerpAPI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")  #C\n",
    "\n",
    "def search_product_urls(search_key: str, num_results: int = 5) -> list[dict]:  #D\n",
    "    \"\"\"Search for product page candidates using SerpAPI.\"\"\"\n",
    "    params = {  #E\n",
    "        \"q\": search_key,\n",
    "        \"api_key\": SERPAPI_KEY,\n",
    "        \"num\": num_results,\n",
    "        \"engine\": \"google\",\n",
    "    }\n",
    "    resp = requests.get(\"https://serpapi.com/search\", params=params)  #F\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    \n",
    "    candidates = []  #G\n",
    "    for result in data.get(\"organic_results\", []):\n",
    "        candidates.append({\n",
    "            \"title\": result.get(\"title\", \"\"),\n",
    "            \"url\": result.get(\"link\", \"\"),\n",
    "            \"snippet\": result.get(\"snippet\", \"\"),\n",
    "            \"position\": result.get(\"position\", 0),\n",
    "        })\n",
    "    return candidates  #H\n",
    "\n",
    "# Example usage\n",
    "search_query = \"GORUCK GR1 26L\"\n",
    "print(f\"\\nSearching for: '{search_query}'\\n\")\n",
    "\n",
    "candidates = search_product_urls(search_query)  #I\n",
    "\n",
    "# Display as DataFrame\n",
    "df_candidates = pd.DataFrame(candidates)\n",
    "print(f\"Found {len(candidates)} candidate URLs:\\n\")\n",
    "display(df_candidates[['position', 'title', 'url']])\n",
    "\n",
    "print(f\"\\n✓ {len(candidates)} URLs ready for AI ranking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_4_2",
   "metadata": {},
   "source": [
    "## 11.4.2 - Listing 11.2: Rank URLs with AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "listing_11_2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISTING 11.2: Rank URLs with AI\n",
      "============================================================\n",
      "\n",
      "Asking AI to rank 3 URLs...\n",
      "\n",
      "Best URL (Confidence: High):\n",
      "https://www.goruck.com/products/gr1-usa?srsltid=AfmBOoowHdfC_HUysWWLQaYEPMbwPPSxpf-VC1fdajBevE14xn5Y61xK\n",
      "\n",
      "Reasoning:\n",
      "The URL from Position 2 is the best choice for extracting structured product data. This page appears to be a direct product page from GORUCK's official website, likely containing detailed information about the GR1 26L rucksack, such as name, price, description, possibly weight, and images. It is more likely to have stable and detailed content specific to this exact product model compared to the URL in Position 1, which seems to lead to a collection or listing page. The URL in Position 3 is from Reddit, which is a discussion forum and is not suitable for structured data extraction.\n",
      "\n",
      "✓ Best URL selected for extraction\n"
     ]
    }
   ],
   "source": [
    "import openai  #A\n",
    "from pydantic import BaseModel  #B\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.2: Rank URLs with AI\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class URLRanking(BaseModel):  #C\n",
    "    best_url: str\n",
    "    confidence: str  # \"high\", \"medium\", \"low\"\n",
    "    reasoning: str\n",
    "\n",
    "def rank_urls_with_ai(  #D\n",
    "    search_key: str,\n",
    "    candidates: list[dict],\n",
    "    model: str = \"gpt-4o\",\n",
    ") -> URLRanking:\n",
    "    \"\"\"Use an LLM to pick the best product page from search results.\"\"\"\n",
    "    \n",
    "    candidate_text = \"\"  #E\n",
    "    for c in candidates:\n",
    "        candidate_text += (\n",
    "            f\"Position {c['position']}:\\n\"\n",
    "            f\"  Title: {c['title']}\\n\"\n",
    "            f\"  URL: {c['url']}\\n\"\n",
    "            f\"  Snippet: {c['snippet']}\\n\\n\"\n",
    "        )\n",
    "    \n",
    "    system_prompt = \"\"\"You are a data engineering assistant helping build a product database.\n",
    "Given a product search key and a list of candidate URLs from search results,\n",
    "pick the single best URL for extracting structured product data.\n",
    "\n",
    "Prefer:\n",
    "1. Manufacturer or official brand pages\n",
    "2. Pages likely to contain: product name, price, description, weight, images\n",
    "3. Individual product pages over category or listing pages\n",
    "4. Stable URLs over session-specific or filtered URLs\n",
    "\n",
    "Avoid:\n",
    "- Review sites, forums, Reddit threads\n",
    "- Retailer pages when a manufacturer page is available\n",
    "- Category pages that list multiple products\n",
    "\n",
    "Return the best URL, your confidence level, and a brief explanation.\"\"\"  #F\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Product: {search_key}\\n\\nCandidate URLs:\\n{candidate_text}\"\n",
    "    )  #G\n",
    "    \n",
    "    response = openai.beta.chat.completions.parse(  #H\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        response_format=URLRanking,  #I\n",
    "    )\n",
    "    return response.choices[0].message.parsed  #J\n",
    "\n",
    "# Example usage\n",
    "print(f\"\\nAsking AI to rank {len(candidates)} URLs...\\n\")\n",
    "ranking = rank_urls_with_ai(search_query, candidates)  #K\n",
    "\n",
    "# Display results\n",
    "print(f\"Best URL (Confidence: {ranking.confidence}):\")\n",
    "print(f\"{ranking.best_url}\\n\")\n",
    "print(f\"Reasoning:\")\n",
    "print(f\"{ranking.reasoning}\\n\")\n",
    "print(f\"✓ Best URL selected for extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_5_1",
   "metadata": {},
   "source": [
    "## 11.5.1 - Listing 11.3: Aggressive HTML Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "listing_11_3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISTING 11.3: Aggressive HTML Cleaning\n",
      "============================================================\n",
      "\n",
      "Fetching HTML from: https://www.goruck.com/products/gr1-usa?srsltid=AfmBOoowHdfC_HUysWWLQaYEPMbwPPSxpf-VC1fdajBevE14xn5Y61xK\n",
      "\n",
      "Cleaning Results:\n",
      "  Raw HTML:    2,032,630 characters\n",
      "  Cleaned:     20,419 characters\n",
      "  Reduction:   99.0%\n",
      "\n",
      "First 200 characters of cleaned text:\n",
      "GR1 | GORUCK Skip to content Presidents Day Sale | Steals, Deals & Bundles FREE RUCK PLATES WITH BASIC RUCKER FREE CURVED PLATES WITH RUCKING WEIGHT VEST FREE USA Shipping for GORUCK Tribe Members Pro...\n",
      "\n",
      "✓ HTML cleaned and ready for extraction\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup  #A\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.3: Aggressive HTML Cleaning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "REMOVE_TAGS = [  #B\n",
    "    \"script\", \"style\", \"nav\", \"footer\", \"header\",\n",
    "    \"iframe\", \"noscript\", \"svg\", \"form\",\n",
    "]\n",
    "\n",
    "REMOVE_CLASSES = [  #C\n",
    "    \"breadcrumb\", \"related-products\", \"recently-viewed\",\n",
    "    \"newsletter\", \"cookie-banner\", \"site-footer\",\n",
    "    \"site-header\", \"cart-drawer\", \"search-modal\",\n",
    "    \"review\", \"reviews\", \"ratings\",\n",
    "]\n",
    "\n",
    "def clean_html_aggressive(html: str) -> str:  #D\n",
    "    \"\"\"Remove non-product HTML elements to reduce noise and token count.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted tags entirely\n",
    "    for tag_name in REMOVE_TAGS:  #E\n",
    "        for element in soup.find_all(tag_name):\n",
    "            element.decompose()\n",
    "    \n",
    "    # Remove elements by class name patterns\n",
    "    for class_pattern in REMOVE_CLASSES:  #F\n",
    "        for element in soup.find_all(\n",
    "            class_=lambda c: c and class_pattern in \" \".join(c).lower()\n",
    "        ):\n",
    "            element.decompose()\n",
    "    \n",
    "    # Remove empty elements\n",
    "    for element in soup.find_all():  #G\n",
    "        if not element.get_text(strip=True) and not element.find(\"img\"):\n",
    "            element.decompose()\n",
    "    \n",
    "    clean_text = \" \".join(soup.stripped_strings)  #H\n",
    "    return clean_text\n",
    "\n",
    "# Example usage - fetch the best URL from previous step\n",
    "print(f\"\\nFetching HTML from: {ranking.best_url}\\n\")\n",
    "raw_html = requests.get(ranking.best_url).text  #I\n",
    "clean = clean_html_aggressive(raw_html)\n",
    "\n",
    "# Display cleaning stats\n",
    "reduction_pct = (1 - len(clean) / len(raw_html)) * 100\n",
    "print(f\"Cleaning Results:\")\n",
    "print(f\"  Raw HTML:    {len(raw_html):,} characters\")\n",
    "print(f\"  Cleaned:     {len(clean):,} characters\")\n",
    "print(f\"  Reduction:   {reduction_pct:.1f}%\\n\")\n",
    "print(f\"First 200 characters of cleaned text:\")\n",
    "print(f\"{clean[:200]}...\\n\")\n",
    "print(f\"✓ HTML cleaned and ready for extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_5_2",
   "metadata": {},
   "source": [
    "## 11.5.2 - Listing 11.4: AI Content Triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "listing_11_4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISTING 11.4: AI Content Triage\n",
      "============================================================\n",
      "\n",
      "Triaging 10 text blocks...\n",
      "\n",
      "Triage Results:\n",
      "  Total blocks:          10\n",
      "  Product sections:      []\n",
      "  Non-product sections:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "\n",
      "✓ Content classified for targeted extraction\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel  #A\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.4: AI Content Triage\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class ContentTriage(BaseModel):  #B\n",
    "    product_sections: list[int]\n",
    "    non_product_sections: list[int]\n",
    "\n",
    "def triage_content(text_blocks: list[str], model: str = \"gpt-4o-mini\") -> ContentTriage:  #C\n",
    "    \"\"\"Use a lightweight LLM to classify text blocks as product or non-product.\"\"\"\n",
    "    blocks_text = \"\"  #D\n",
    "    for i, block in enumerate(text_blocks):\n",
    "        blocks_text += f\"[Block {i}]: {block[:150]}...\\n\\n\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a data engineering assistant. Given a list of text blocks\n",
    "from a product web page, classify each block as either product-relevant or not.\n",
    "\n",
    "Product-relevant blocks contain: product name, price, description, specifications,\n",
    "weight, dimensions, materials, features, or sizing information.\n",
    "\n",
    "Non-product blocks contain: navigation, shipping info, return policies, reviews,\n",
    "promotional banners, newsletter signups, or generic site content.\n",
    "\n",
    "Return two lists: product_sections (the block numbers that contain product data)\n",
    "and non_product_sections (everything else).\"\"\"  #E\n",
    "    \n",
    "    response = openai.beta.chat.completions.parse(  #F\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": blocks_text},\n",
    "        ],\n",
    "        response_format=ContentTriage,\n",
    "    )\n",
    "    return response.choices[0].message.parsed  #G\n",
    "\n",
    "# Example - split cleaned text into blocks\n",
    "blocks = [s.strip() for s in clean.split('.') if s.strip()][:10]  # First 10 sentences\n",
    "print(f\"\\nTriaging {len(blocks)} text blocks...\\n\")\n",
    "\n",
    "triage = triage_content(blocks)\n",
    "\n",
    "print(f\"Triage Results:\")\n",
    "print(f\"  Total blocks:          {len(blocks)}\")\n",
    "print(f\"  Product sections:      {triage.product_sections}\")\n",
    "print(f\"  Non-product sections:  {triage.non_product_sections}\\n\")\n",
    "print(f\"✓ Content classified for targeted extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_6_1",
   "metadata": {},
   "source": [
    "## 11.6.1 - Listing 11.5: Define Extraction Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "listing_11_5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISTING 11.5: Define Extraction Schema\n",
      "============================================================\n",
      "\n",
      "ProductExtraction Schema:\n",
      "  • product_name         (required)\n",
      "  • brand_name           (required)\n",
      "  • description          (optional)\n",
      "  • price                (optional)\n",
      "  • weight               (optional)\n",
      "  • primary_image_url    (optional)\n",
      "  • category             (optional)\n",
      "\n",
      "✓ Schema defined with 7 fields\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field  #A\n",
    "from typing import Optional  #B\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.5: Define Extraction Schema\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class ProductExtraction(BaseModel):  #C\n",
    "    \"\"\"Schema for extracting product data from web page content.\"\"\"\n",
    "    product_name: str = Field(  #D\n",
    "        description=\"Full product name as shown on the page\"\n",
    "    )\n",
    "    brand_name: str = Field(  #E\n",
    "        description=\"Manufacturer or brand name\"\n",
    "    )\n",
    "    description: Optional[str] = Field(  #F\n",
    "        default=None,\n",
    "        description=\"Product description, typically 1-3 sentences\"\n",
    "    )\n",
    "    price: Optional[str] = Field(  #G\n",
    "        default=None,\n",
    "        description=\"Current retail price including currency symbol\"\n",
    "    )\n",
    "    weight: Optional[str] = Field(  #H\n",
    "        default=None,\n",
    "        description=\"Product weight with unit (e.g., '2.5 lbs', '1.1 kg')\"\n",
    "    )\n",
    "    primary_image_url: Optional[str] = Field(  #I\n",
    "        default=None,\n",
    "        description=\"URL of the main product image\"\n",
    "    )\n",
    "    category: Optional[str] = Field(  #J\n",
    "        default=None,\n",
    "        description=\"Product category (e.g., backpack, tent, sleeping bag)\"\n",
    "    )\n",
    "\n",
    "# Display schema\n",
    "print(\"\\nProductExtraction Schema:\")\n",
    "for field_name, field_info in ProductExtraction.model_fields.items():\n",
    "    required = \"required\" if field_info.is_required() else \"optional\"\n",
    "    print(f\"  • {field_name:20s} ({required})\")\n",
    "\n",
    "print(f\"\\n✓ Schema defined with {len(ProductExtraction.model_fields)} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_6_2",
   "metadata": {},
   "source": [
    "## 11.6.2 - Listing 11.6: AI Product Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "listing_11_6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISTING 11.6: AI Product Extraction\n",
      "============================================================\n",
      "\n",
      "Extracting product data from cleaned HTML...\n",
      "\n",
      "Extracted Product Data:\n",
      "  Name:        GR1 USA - Cordura\n",
      "  Brand:       GORUCK\n",
      "  Price:       $335.00\n",
      "  Weight:      2.8 LBS\n",
      "  Category:    backpack\n",
      "  Description: GR1 was built to thrive in Baghdad and NYC and has been tested and proven by Green Berets to meet th...\n",
      "\n",
      "✓ Product data extracted successfully\n"
     ]
    }
   ],
   "source": [
    "import openai  #A\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.6: AI Product Extraction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "EXTRACTION_PROMPT = \"\"\"You are a product data extraction assistant for a data engineering pipeline.\n",
    "\n",
    "Given the text content of a product web page, extract the following fields accurately:\n",
    "- product_name: The full product name as displayed on the page\n",
    "- brand_name: The manufacturer or brand\n",
    "- description: A concise product description (1-3 sentences)\n",
    "- price: The current retail price with currency symbol\n",
    "- weight: The product weight with unit if available\n",
    "- primary_image_url: The URL of the main product image if found in the text\n",
    "- category: The product category (backpack, tent, sleeping bag, headlamp, etc.)\n",
    "\n",
    "Rules:\n",
    "- Only extract information that is explicitly present in the text\n",
    "- Use null for any field you cannot find or confidently determine\n",
    "- Do not guess or fabricate values\n",
    "- For price, use the current or sale price, not the original price if both are shown\n",
    "- For weight, include the unit (lbs, oz, kg, g)\n",
    "- For category, use a simple label based on what the product is\"\"\"  #B\n",
    "\n",
    "def extract_product_with_ai(  #C\n",
    "    cleaned_text: str,\n",
    "    model: str = \"gpt-4o\",\n",
    ") -> ProductExtraction:\n",
    "    \"\"\"Extract product fields from cleaned page text using an LLM.\"\"\"\n",
    "    response = openai.beta.chat.completions.parse(  #D\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EXTRACTION_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": cleaned_text[:8000]},  #E\n",
    "        ],\n",
    "        response_format=ProductExtraction,  #F\n",
    "    )\n",
    "    return response.choices[0].message.parsed  #G\n",
    "\n",
    "# Example usage\n",
    "print(f\"\\nExtracting product data from cleaned HTML...\\n\")\n",
    "product = extract_product_with_ai(clean)  #H\n",
    "\n",
    "# Display extraction results\n",
    "print(f\"Extracted Product Data:\")\n",
    "print(f\"  Name:        {product.product_name}\")\n",
    "print(f\"  Brand:       {product.brand_name}\")\n",
    "print(f\"  Price:       {product.price}\")\n",
    "print(f\"  Weight:      {product.weight}\")\n",
    "print(f\"  Category:    {product.category}\")\n",
    "if product.description:\n",
    "    desc_preview = product.description[:100] + \"...\" if len(product.description) > 100 else product.description\n",
    "    print(f\"  Description: {desc_preview}\")\n",
    "\n",
    "print(f\"\\n✓ Product data extracted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_6_3",
   "metadata": {},
   "source": [
    "## 11.6.3 - Listing 11.7: Manual vs AI Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "listing_11_7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISTING 11.7: Manual vs AI Extraction Comparison\n",
      "============================================================\n",
      "\n",
      "Comparing manual (CSS selectors) vs AI extraction...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field</th>\n",
       "      <th>Manual (Ch 10)</th>\n",
       "      <th>AI (Ch 11)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>product_name</td>\n",
       "      <td>GR1 USA - Cordura</td>\n",
       "      <td>GR1 USA - Cordura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brand_name</td>\n",
       "      <td>GORUCK</td>\n",
       "      <td>GORUCK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price</td>\n",
       "      <td>$335.00</td>\n",
       "      <td>$335.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weight</td>\n",
       "      <td>None</td>\n",
       "      <td>2.8 LBS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>category</td>\n",
       "      <td>None</td>\n",
       "      <td>backpack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>description</td>\n",
       "      <td>None</td>\n",
       "      <td>GR1 was built to thrive in Baghdad and NYC and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Field     Manual (Ch 10)  \\\n",
       "0  product_name  GR1 USA - Cordura   \n",
       "1    brand_name             GORUCK   \n",
       "2         price            $335.00   \n",
       "3        weight               None   \n",
       "4      category               None   \n",
       "5   description               None   \n",
       "\n",
       "                                          AI (Ch 11)  \n",
       "0                                  GR1 USA - Cordura  \n",
       "1                                             GORUCK  \n",
       "2                                            $335.00  \n",
       "3                                            2.8 LBS  \n",
       "4                                           backpack  \n",
       "5  GR1 was built to thrive in Baghdad and NYC and...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison Summary:\n",
      "  Manual approach: 3/6 fields populated\n",
      "  AI approach:     6/6 fields populated\n",
      "\n",
      "✓ AI extracted 3 additional fields\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  #A\n",
    "from bs4 import BeautifulSoup  #B\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.7: Manual vs AI Extraction Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def extract_manual_goruck(html: str) -> dict:  #C\n",
    "    \"\"\"Manual extraction using Chapter 10's CSS selector approach.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    title_el = soup.find(\"h1\")\n",
    "    title = title_el.get_text(\" \", strip=True) if title_el else None\n",
    "    \n",
    "    price_el = (\n",
    "        soup.select_one(\n",
    "            \"div.product-block__price span.price-item--sale.price-item--last\"\n",
    "        )\n",
    "        or soup.select_one(\"div.product-block__price span.price-item--regular\")\n",
    "    )\n",
    "    price = price_el.get_text(\" \", strip=True) if price_el else None\n",
    "    \n",
    "    return {\n",
    "        \"product_name\": title,\n",
    "        \"brand_name\": \"GORUCK\",  # hardcoded for this site\n",
    "        \"price\": price,\n",
    "        \"weight\": None,  # manual approach did not extract this\n",
    "        \"category\": None,  # manual approach did not extract this\n",
    "        \"description\": None,  # manual approach did not extract this\n",
    "    }\n",
    "\n",
    "# Run both approaches on the same page\n",
    "print(\"\\nComparing manual (CSS selectors) vs AI extraction...\\n\")\n",
    "manual_result = extract_manual_goruck(raw_html)  #E\n",
    "ai_result = product  #F (from previous cell)\n",
    "\n",
    "# Build comparison table\n",
    "comparison = pd.DataFrame({  #G\n",
    "    \"Field\": [\"product_name\", \"brand_name\", \"price\", \"weight\",\n",
    "              \"category\", \"description\"],\n",
    "    \"Manual (Ch 10)\": [\n",
    "        manual_result[\"product_name\"],\n",
    "        manual_result[\"brand_name\"],\n",
    "        manual_result[\"price\"],\n",
    "        manual_result[\"weight\"],\n",
    "        manual_result[\"category\"],\n",
    "        manual_result[\"description\"],\n",
    "    ],\n",
    "    \"AI (Ch 11)\": [\n",
    "        ai_result.product_name,\n",
    "        ai_result.brand_name,\n",
    "        ai_result.price,\n",
    "        ai_result.weight,\n",
    "        ai_result.category,\n",
    "        ai_result.description[:50] + \"...\" if ai_result.description else None,\n",
    "    ],\n",
    "})\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# Count populated fields\n",
    "manual_populated = sum(1 for v in manual_result.values() if v is not None)\n",
    "ai_populated = sum(1 for v in [ai_result.product_name, ai_result.brand_name, ai_result.price, \n",
    "                                 ai_result.weight, ai_result.category, ai_result.description] if v is not None)\n",
    "\n",
    "print(f\"\\nComparison Summary:\")\n",
    "print(f\"  Manual approach: {manual_populated}/6 fields populated\")\n",
    "print(f\"  AI approach:     {ai_populated}/6 fields populated\")\n",
    "print(f\"\\n✓ AI extracted {ai_populated - manual_populated} additional fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_7_1",
   "metadata": {},
   "source": [
    "## 11.7.1 - Listing 11.8: Multi-Site Batch Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "listing_11_8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISTING 11.8: Multi-Site Batch Extraction\n",
      "============================================================\n",
      "\n",
      "Processing 2 products across multiple sites...\n",
      "\n",
      "  • GORUCK GR1 26L...\n",
      "  • Osprey Atmos AG 65...\n",
      "\n",
      "Batch Extraction Results:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>search_key</th>\n",
       "      <th>status</th>\n",
       "      <th>product_name</th>\n",
       "      <th>price</th>\n",
       "      <th>weight</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GORUCK GR1 26L</td>\n",
       "      <td>success</td>\n",
       "      <td>GR1</td>\n",
       "      <td>$335.00</td>\n",
       "      <td>2.8 LBS</td>\n",
       "      <td>backpack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Osprey Atmos AG 65</td>\n",
       "      <td>success</td>\n",
       "      <td>Osprey Aether Plus 70 Backpack</td>\n",
       "      <td>$399.95</td>\n",
       "      <td>5.2 lbs</td>\n",
       "      <td>backpack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           search_key   status                    product_name    price  \\\n",
       "0      GORUCK GR1 26L  success                             GR1  $335.00   \n",
       "1  Osprey Atmos AG 65  success  Osprey Aether Plus 70 Backpack  $399.95   \n",
       "\n",
       "    weight  category  \n",
       "0  2.8 LBS  backpack  \n",
       "1  5.2 lbs  backpack  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ 2/2 products extracted successfully\n"
     ]
    }
   ],
   "source": [
    "import time  #A\n",
    "import pandas as pd  #B\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.8: Multi-Site Batch Extraction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_products = [  #D\n",
    "    {\"name\": \"GORUCK GR1 26L\",\n",
    "     \"url\": \"https://www.goruck.com/products/gr1\"},\n",
    "    {\"name\": \"Osprey Atmos AG 65\",\n",
    "     \"url\": \"https://www.osprey.com/us/en/product/atmos-ag-65-ATMOS65S23.html\"},\n",
    "]\n",
    "\n",
    "print(f\"\\nProcessing {len(test_products)} products across multiple sites...\\n\")\n",
    "\n",
    "results = []  #E\n",
    "for product in test_products:\n",
    "    record = {  #F\n",
    "        \"search_key\": product[\"name\"],\n",
    "        \"url\": product[\"url\"],\n",
    "        \"status\": \"error\",\n",
    "    }\n",
    "    try:\n",
    "        print(f\"  • {product['name']}...\")\n",
    "        raw_html = requests.get(product[\"url\"]).text  #G\n",
    "        cleaned = clean_html_aggressive(raw_html)\n",
    "        extraction = extract_product_with_ai(cleaned)  #H\n",
    "        \n",
    "        record[\"product_name\"] = extraction.product_name\n",
    "        record[\"brand_name\"] = extraction.brand_name\n",
    "        record[\"price\"] = extraction.price\n",
    "        record[\"weight\"] = extraction.weight\n",
    "        record[\"category\"] = extraction.category\n",
    "        record[\"description\"] = (\n",
    "            extraction.description[:60] + \"...\"\n",
    "            if extraction.description\n",
    "            else None\n",
    "        )\n",
    "        record[\"status\"] = \"success\"  #I\n",
    "    except Exception as e:\n",
    "        record[\"status\"] = f\"error: {type(e).__name__}\"  #J\n",
    "        print(f\"    Error: {type(e).__name__}\")\n",
    "    \n",
    "    results.append(record)\n",
    "    time.sleep(2)  #K\n",
    "\n",
    "out = pd.DataFrame(results)  #L\n",
    "print(f\"\\nBatch Extraction Results:\\n\")\n",
    "display(out[[\"search_key\", \"status\", \"product_name\", \"price\", \"weight\", \"category\"]])\n",
    "\n",
    "success_count = (out[\"status\"] == \"success\").sum()\n",
    "print(f\"\\n✓ {success_count}/{len(test_products)} products extracted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section_11_8",
   "metadata": {},
   "source": [
    "## 11.8 - Listing 11.9: Token & Cost Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "listing_11_9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LISTING 11.9: Token & Cost Estimation\n",
      "============================================================\n",
      "\n",
      "Estimating costs for cleaned HTML (model: gpt-4o)...\n",
      "\n",
      "Token & Cost Analysis:\n",
      "  Input tokens:          5,121\n",
      "  Output tokens (est):   300\n",
      "  Cost per page:         $0.0158\n",
      "  Cost for 450 products: $7.11\n",
      "\n",
      "✓ Cost estimate: ~$7.11 for full pipeline\n"
     ]
    }
   ],
   "source": [
    "import tiktoken  #A\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LISTING 11.9: Token & Cost Estimation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def estimate_extraction_cost(  #B\n",
    "    text: str,\n",
    "    model: str = \"gpt-4o\",\n",
    "    output_tokens: int = 300,\n",
    ") -> dict:\n",
    "    \"\"\"Estimate the token count and cost for an extraction call.\"\"\"\n",
    "    encoder = tiktoken.encoding_for_model(model)  #C\n",
    "    input_tokens = len(encoder.encode(text))  #D\n",
    "    \n",
    "    # Pricing as of mid-2025 (check docs for current rates)\n",
    "    pricing = {  #E\n",
    "        \"gpt-4o\": {\"input\": 2.50 / 1_000_000, \"output\": 10.00 / 1_000_000},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15 / 1_000_000, \"output\": 0.60 / 1_000_000},\n",
    "    }\n",
    "    \n",
    "    rates = pricing.get(model, pricing[\"gpt-4o\"])  #F\n",
    "    input_cost = input_tokens * rates[\"input\"]\n",
    "    output_cost = output_tokens * rates[\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {  #G\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens_est\": output_tokens,\n",
    "        \"input_cost\": round(input_cost, 6),\n",
    "        \"output_cost\": round(output_cost, 6),\n",
    "        \"total_cost\": round(total_cost, 6),\n",
    "    }\n",
    "\n",
    "# Estimate for a single product page\n",
    "print(f\"\\nEstimating costs for cleaned HTML (model: gpt-4o)...\\n\")\n",
    "cost = estimate_extraction_cost(clean)  #H\n",
    "\n",
    "print(f\"Token & Cost Analysis:\")\n",
    "print(f\"  Input tokens:          {cost['input_tokens']:,}\")\n",
    "print(f\"  Output tokens (est):   {cost['output_tokens_est']:,}\")\n",
    "print(f\"  Cost per page:         ${cost['total_cost']:.4f}\")\n",
    "print(f\"  Cost for 450 products: ${cost['total_cost'] * 450:.2f}\\n\")\n",
    "print(f\"✓ Cost estimate: ~${cost['total_cost'] * 450:.2f} for full pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9a076-4201-42dd-a812-6e830194e3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
