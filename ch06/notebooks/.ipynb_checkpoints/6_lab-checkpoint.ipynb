{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d568cc6-250a-4c7f-ad84-6fae13879ebb",
   "metadata": {},
   "source": [
    "# Chapter 6 Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3be3c-5404-41aa-8f69-40ad3cb30d24",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## üß™ Lab: Cleaning Up Bill‚Äôs Surf Shop Orders\n",
    "\n",
    "Bill runs a gnarly surf shop on the East Coast, but he‚Äôs not exactly a data guy. His order file is riddled with issues that make it hard to analyze or trust. Your task is to take this messy dataset and bring it up to par.\n",
    "\n",
    "This lab builds directly on the skills you practiced in this chapter: defining data quality rules, implementing them in Python, and then re-implementing them using a structured Chat Completions API prompt. Your goal is to write both a Python solution and an AI-driven solution that produce the same cleaned dataset.\n",
    "\n",
    "You‚Äôll be working with the file `bills_surf_shop_orders.csv` (located in the `ch06/setup/` folder).\n",
    "\n",
    "---\n",
    "\n",
    "### üßº Fields to Clean and Standardize\n",
    "\n",
    "Below are the fields in the dataset, along with the known issues you‚Äôll need to detect and resolve:\n",
    "\n",
    "---\n",
    "\n",
    "#### `order_id`\n",
    "\n",
    "* ‚úÖ This should be clean already ‚Äî it‚Äôs just the row index. No action needed.\n",
    "\n",
    "---\n",
    "\n",
    "#### `email`\n",
    "\n",
    "* ‚úÖ Valid values look like standard emails.\n",
    "* ‚ùå Some entries are badly formatted (`example@.co`, `555-1234`, `None`, etc.).\n",
    "* üîß Detect and optionally flag or remove invalid email formats.\n",
    "\n",
    "---\n",
    "\n",
    "#### `age`\n",
    "\n",
    "* ‚úÖ Should be a number between 18 and 100.\n",
    "* ‚ùå Some rows are missing `age`.\n",
    "* üîß Detect nulls and consider strategies for imputation or row exclusion.\n",
    "\n",
    "---\n",
    "\n",
    "#### `purchase_amount`\n",
    "\n",
    "* ‚úÖ Most rows have a positive number here.\n",
    "* ‚ùå Some rows are missing or contain negative values.\n",
    "* üîß Identify negative amounts or nulls ‚Äî decide whether to drop, impute, or flag.\n",
    "\n",
    "---\n",
    "\n",
    "#### `customer_id`\n",
    "\n",
    "* ‚úÖ Should be consistent per customer (used for joins).\n",
    "* üîç No cleaning needed for now ‚Äî but check for nulls if you want bonus points.\n",
    "\n",
    "---\n",
    "\n",
    "#### `store_location`\n",
    "\n",
    "* ‚úÖ Should be one of: `Florida`, `North Carolina`, `South Carolina`, `New Jersey`, `Rhode Island`, or their abbreviations: `FL`, `NC`, `SC`, `NJ`, `RI`.\n",
    "* ‚ùå Some rows contain invalid entries (e.g., `Flrda`, `Carolinas`, etc.).\n",
    "* üîß Create a mapping and standardize locations.\n",
    "\n",
    "---\n",
    "\n",
    "#### `optional_note`\n",
    "\n",
    "* ‚ùå This column is always `null` and not useful.\n",
    "* üîß Drop this column entirely.\n",
    "\n",
    "---\n",
    "\n",
    "#### `purchase_date`\n",
    "\n",
    "* ‚ùå Values come in different formats (`12/31/2023`, `2023/01/01`, `15-01-2023`, etc.).\n",
    "* üîß Standardize to the format `YYYY-MM-DD`.\n",
    "\n",
    "---\n",
    "\n",
    "#### `SKU`\n",
    "\n",
    "* ‚úÖ Valid format is three uppercase letters followed by three digits (e.g., `SUR123`).\n",
    "* ‚ùå Some entries are missing or malformed.\n",
    "* üîß Extract and validate the pattern.\n",
    "\n",
    "---\n",
    "\n",
    "#### `description`\n",
    "\n",
    "* ‚úÖ Generally good quality text (e.g., `blue surfboard - 6ft soft top`).\n",
    "* ‚ùå Some entries are missing or inconsistently formatted.\n",
    "* üîß Consider truncating to 20 characters for reporting purposes.\n",
    "* üó∫ Map the description field to a high-level product category (e.g., \"board\", \"wetsuit\", \"accessory\") using a simple dictionary. You can define this yourself based on values you observe in the dataset\n",
    "\n",
    "---\n",
    "\n",
    "#### `first_name` + `last_name`\n",
    "\n",
    "* ‚úÖ Usually clean.\n",
    "* üîß Combine into a `full_name` column.\n",
    "\n",
    "---\n",
    "\n",
    "#### `product_name`\n",
    "\n",
    "* ‚úÖ Each entry corresponds to a surf-related item.\n",
    "* üîß Map these to standard product categories (e.g., `surfboard`, `wetsuit`, `accessory`).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### üîß Your Task\n",
    "\n",
    "#### Step 1: Clean the Dataset Using Python\n",
    "\n",
    "Use only `pandas` and standard Python functions to clean the dataset according to the rules above. You may find yourself using:\n",
    "\n",
    "* `.apply()` or `.map()`\n",
    "* `regex` matching\n",
    "* `dropna()` or `drop()`\n",
    "* custom functions for standardizing formats\n",
    "\n",
    "Make sure your cleaned DataFrame has:\n",
    "\n",
    "* A standardized format across all fields\n",
    "* No invalid rows for `purchase_amount` or `email`\n",
    "* A `full_name` column\n",
    "* No `optional_note` column\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Clean the Dataset Using Chat Completions\n",
    "\n",
    "Now it's time to clean the same dataset using the **structured response format** technique you learned in Listings 6.4 and 6.6. The idea is to define exactly what kind of cleaned output you want, use a `BaseModel` data class to structure the response, and have the model return parsed values you can drop directly into your DataFrame.\n",
    "\n",
    "#### ‚öôÔ∏è Your AI-Powered Cleaning Workflow:\n",
    "\n",
    "1. **Define a new data class** using `pydantic.BaseModel` that includes all the fields you want to clean or standardize (e.g., `normalized_date`, `cleaned_sku`, `full_name`, `product_category`, etc.).\n",
    "\n",
    "2. **Write a detailed prompt** explaining the cleaning rules. For each field, describe what valid data looks like and what the model should return when it's missing, invalid, or malformed.\n",
    "\n",
    "   Your prompt should ask the model to:\n",
    "\n",
    "   * Normalize `purchase_date` to `YYYY-MM-DD` format\n",
    "   * Clean or nullify invalid `SKU` values\n",
    "   * Truncate `description` to 20 characters\n",
    "   * Combine `first_name` and `last_name` into `full_name`\n",
    "   * Map `description` or `product_name` to a standardized `product_category`\n",
    "   * Return `None` for invalid emails or negative/missing purchase amounts\n",
    "\n",
    "3. **‚ùó Important: Process each record individually.**\n",
    "\n",
    "   In earlier sections, we passed the entire dataset at once using `df.to_dict(orient=\"records\")`. However, with larger or messy datasets, this can cause errors if the model skips records or returns inconsistent response lengths.\n",
    "\n",
    "   **Instead, loop through the dataset one row at a time.** Use `tqdm` to show progress and feed each record through the API call with its own message and prompt. This gives you:\n",
    "\n",
    "   * Stable response lengths (one output per input)\n",
    "   * Cleaner debugging when something goes wrong\n",
    "   * A safer way to accumulate results row by row\n",
    "\n",
    "4. **Build your final DataFrame** from the list of cleaned records. Drop any rows that are still missing critical fields like `email` or `purchase_amount`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üí¨ Discussion Questions\n",
    "\n",
    "* Which approach was easier to build and test?\n",
    "* Which was more flexible when you needed to change rules?\n",
    "* How confident are you in the AI-generated cleaning?\n",
    "* Could you imagine building a reusable framework from this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91ca25-fc1f-4281-b435-f300f9d9c661",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feel free to use the provided csv in ../setup. For an extra challenge you can generate a new file with similar attributes using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "200a46f6-7af2-4989-ab1b-c12e897c19e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Step 1: Generate 50 consistent customer profiles\n",
    "num_customers = 50\n",
    "customers = []\n",
    "for _ in range(num_customers):\n",
    "    first = fake.first_name()\n",
    "    last = fake.last_name()\n",
    "    email = f\"{first.lower()}.{last.lower()}@{fake.free_email_domain()}\"\n",
    "    age = random.randint(18, 65)\n",
    "    customer_id = fake.uuid4()\n",
    "    customers.append({\n",
    "        \"customer_id\": customer_id,\n",
    "        \"first_name\": first,\n",
    "        \"last_name\": last,\n",
    "        \"email\": email,\n",
    "        \"age\": age\n",
    "    })\n",
    "\n",
    "# Step 2: Define surf-themed items with fixed SKUs\n",
    "surf_items = [\n",
    "    \"Longboard surfboard\", \"Shortboard surfboard\", \"Soft-top surfboard\",\n",
    "    \"Wetsuit - full\", \"Wetsuit - shorty\", \"Rash guard\",\n",
    "    \"Board shorts\", \"Surf wax\", \"Leash\", \"Fins\",\n",
    "    \"Surf helmet\", \"Dry bag\", \"Waterproof watch\", \"Beach towel\",\n",
    "    \"Surf poncho\", \"Roof rack straps\", \"Wax comb\", \"Traction pad\",\n",
    "    \"Sunscreen\", \"Ear plugs\"\n",
    "]\n",
    "sku_valid = [f\"{fake.lexify('???').upper()}{fake.numerify('###')}\" for _ in surf_items]\n",
    "item_sku_map = dict(zip(surf_items, sku_valid))\n",
    "sku_item_map = {v: k for k, v in item_sku_map.items()}\n",
    "\n",
    "sku_invalid = ['abc12', '123ABC', 'a12bc3', None]\n",
    "\n",
    "# Step 3: Set up locations and date formats\n",
    "store_locations = ['New York', 'NY', 'New Jersey', 'NJ', 'Florida', 'FL', 'Massachusetts', 'MA', 'Virginia', 'VA']\n",
    "valid_dates = ['12/31/2023', '2023-01-01', '01-15-2023']\n",
    "invalid_dates = ['31/12/2023', '2023/01/01', '15-01-2023']\n",
    "\n",
    "invalid_emails = ['example@.co', 'example@', '555-1234', None]\n",
    "\n",
    "# Step 4: Build 100 order records\n",
    "num_orders = 100\n",
    "records = []\n",
    "for i in range(1, num_orders + 1):\n",
    "    customer = random.choice(customers)\n",
    "    email = random.choice(invalid_emails) if random.random() < 0.25 else customer['email']\n",
    "    age = None if random.random() < 0.1 else customer['age']\n",
    "    purchase_amount = round(random.uniform(25, 800), 2) if random.random() > 0.15 else random.choice([None, -round(random.uniform(10, 200), 2)])\n",
    "    location = random.choice(store_locations)\n",
    "    purchase_date = random.choice(valid_dates) if random.random() > 0.2 else random.choice(invalid_dates)\n",
    "\n",
    "    if random.random() < 0.2:\n",
    "        sku = random.choice(sku_invalid)\n",
    "        description = None\n",
    "    else:\n",
    "        description, sku = random.choice(list(item_sku_map.items()))\n",
    "\n",
    "    records.append([\n",
    "        i, email, age, purchase_amount,\n",
    "        customer['customer_id'], location, None,\n",
    "        purchase_date, sku, description,\n",
    "        customer['first_name'], customer['last_name']\n",
    "    ])\n",
    "\n",
    "# Step 5: Create and export DataFrame\n",
    "df = pd.DataFrame(records, columns=[\n",
    "    \"order_id\", \"email\", \"age\", \"purchase_amount\", \"customer_id\",\n",
    "    \"store_location\", \"optional_note\", \"purchase_date\",\n",
    "    \"SKU\", \"description\", \"first_name\", \"last_name\"\n",
    "])\n",
    "\n",
    "df.to_csv(\"../setup/bills_surf_shop_orders.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33afd80-25b9-4790-b524-db9774227df0",
   "metadata": {},
   "source": [
    "## Step 1: Clean the Dataset Using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568eca27-7269-44e6-9bb2-3039a2ef93bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>email</th>\n",
       "      <th>age</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>store_location</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>SKU</th>\n",
       "      <th>description</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>katie.smith@hotmail.com</td>\n",
       "      <td>38.0</td>\n",
       "      <td>516.46</td>\n",
       "      <td>7e430391-da3b-4ab8-bd2e-63e22a6b97bb</td>\n",
       "      <td>Florida</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>CPT535</td>\n",
       "      <td>Leash</td>\n",
       "      <td>Katie</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Katie Smith</td>\n",
       "      <td>accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>amanda.shaw@hotmail.com</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.93</td>\n",
       "      <td>5ccbe607-4190-49cd-a729-8fdf272fd647</td>\n",
       "      <td>New York</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>ZBA521</td>\n",
       "      <td>Surf wax</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>Shaw</td>\n",
       "      <td>Amanda Shaw</td>\n",
       "      <td>accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>betty.hicks@hotmail.com</td>\n",
       "      <td>43.0</td>\n",
       "      <td>114.53</td>\n",
       "      <td>38f0c359-adce-46ae-b1cd-8dbaa3453a00</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>ICZ284</td>\n",
       "      <td>Wax comb</td>\n",
       "      <td>Betty</td>\n",
       "      <td>Hicks</td>\n",
       "      <td>Betty Hicks</td>\n",
       "      <td>accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>stephanie.sanders@gmail.com</td>\n",
       "      <td>46.0</td>\n",
       "      <td>503.94</td>\n",
       "      <td>68884de5-2364-49a0-a29b-289a836d25e3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Sanders</td>\n",
       "      <td>Stephanie Sanders</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>ashley.smith@yahoo.com</td>\n",
       "      <td>32.0</td>\n",
       "      <td>386.81</td>\n",
       "      <td>f4f04583-8c20-4bfa-a597-c2c2ec7363c8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ashley</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Ashley Smith</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>michelle.miller@gmail.com</td>\n",
       "      <td>30.0</td>\n",
       "      <td>153.66</td>\n",
       "      <td>1fef2f15-047b-4ee6-9723-1ef67983e7a2</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>ALE909</td>\n",
       "      <td>Soft-top surfboard</td>\n",
       "      <td>Michelle</td>\n",
       "      <td>Miller</td>\n",
       "      <td>Michelle Miller</td>\n",
       "      <td>boards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>edward.hunt@hotmail.com</td>\n",
       "      <td>62.0</td>\n",
       "      <td>672.24</td>\n",
       "      <td>41473f03-e37c-4fff-8d91-7a4963d48b8d</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Hunt</td>\n",
       "      <td>Edward Hunt</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>alicia.carroll@gmail.com</td>\n",
       "      <td>25.0</td>\n",
       "      <td>708.20</td>\n",
       "      <td>50f5bcb6-067e-48d1-bf2e-c967887372f7</td>\n",
       "      <td>Florida</td>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>VVL444</td>\n",
       "      <td>Rash guard</td>\n",
       "      <td>Alicia</td>\n",
       "      <td>Carroll</td>\n",
       "      <td>Alicia Carroll</td>\n",
       "      <td>apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>sarah.reeves@yahoo.com</td>\n",
       "      <td>41.0</td>\n",
       "      <td>544.23</td>\n",
       "      <td>4c938301-38ec-491c-9771-f881fa9e5f11</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sarah</td>\n",
       "      <td>Reeves</td>\n",
       "      <td>Sarah Reeves</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>danielle.cooke@hotmail.com</td>\n",
       "      <td>33.0</td>\n",
       "      <td>656.46</td>\n",
       "      <td>2ec7c5e4-5400-4b77-823a-417dd1741fb9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>AAS588</td>\n",
       "      <td>Surf poncho</td>\n",
       "      <td>Danielle</td>\n",
       "      <td>Cooke</td>\n",
       "      <td>Danielle Cooke</td>\n",
       "      <td>apparel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows √ó 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    order_id                        email   age  purchase_amount  \\\n",
       "0          1      katie.smith@hotmail.com  38.0           516.46   \n",
       "3          4      amanda.shaw@hotmail.com  60.0            40.93   \n",
       "4          5      betty.hicks@hotmail.com  43.0           114.53   \n",
       "5          6  stephanie.sanders@gmail.com  46.0           503.94   \n",
       "6          7       ashley.smith@yahoo.com  32.0           386.81   \n",
       "..       ...                          ...   ...              ...   \n",
       "94        95    michelle.miller@gmail.com  30.0           153.66   \n",
       "95        96      edward.hunt@hotmail.com  62.0           672.24   \n",
       "96        97     alicia.carroll@gmail.com  25.0           708.20   \n",
       "97        98       sarah.reeves@yahoo.com  41.0           544.23   \n",
       "98        99   danielle.cooke@hotmail.com  33.0           656.46   \n",
       "\n",
       "                             customer_id store_location purchase_date     SKU  \\\n",
       "0   7e430391-da3b-4ab8-bd2e-63e22a6b97bb        Florida    2023-01-15  CPT535   \n",
       "3   5ccbe607-4190-49cd-a729-8fdf272fd647       New York    2023-01-15  ZBA521   \n",
       "4   38f0c359-adce-46ae-b1cd-8dbaa3453a00  Massachusetts    2023-01-15  ICZ284   \n",
       "5   68884de5-2364-49a0-a29b-289a836d25e3            NaN    2023-12-31     NaN   \n",
       "6   f4f04583-8c20-4bfa-a597-c2c2ec7363c8            NaN    2023-01-15     NaN   \n",
       "..                                   ...            ...           ...     ...   \n",
       "94  1fef2f15-047b-4ee6-9723-1ef67983e7a2  Massachusetts    2023-01-15  ALE909   \n",
       "95  41473f03-e37c-4fff-8d91-7a4963d48b8d     New Jersey    2023-01-15     NaN   \n",
       "96  50f5bcb6-067e-48d1-bf2e-c967887372f7        Florida    2023-12-31  VVL444   \n",
       "97  4c938301-38ec-491c-9771-f881fa9e5f11     New Jersey    2023-01-01     NaN   \n",
       "98  2ec7c5e4-5400-4b77-823a-417dd1741fb9            NaN    2023-01-01  AAS588   \n",
       "\n",
       "           description first_name last_name          full_name  \\\n",
       "0                Leash      Katie     Smith        Katie Smith   \n",
       "3             Surf wax     Amanda      Shaw        Amanda Shaw   \n",
       "4             Wax comb      Betty     Hicks        Betty Hicks   \n",
       "5                  NaN  Stephanie   Sanders  Stephanie Sanders   \n",
       "6                  NaN     Ashley     Smith       Ashley Smith   \n",
       "..                 ...        ...       ...                ...   \n",
       "94  Soft-top surfboard   Michelle    Miller    Michelle Miller   \n",
       "95                 NaN     Edward      Hunt        Edward Hunt   \n",
       "96          Rash guard     Alicia   Carroll     Alicia Carroll   \n",
       "97                 NaN      Sarah    Reeves       Sarah Reeves   \n",
       "98         Surf poncho   Danielle     Cooke     Danielle Cooke   \n",
       "\n",
       "   product_category  \n",
       "0       accessories  \n",
       "3       accessories  \n",
       "4       accessories  \n",
       "5           unknown  \n",
       "6           unknown  \n",
       "..              ...  \n",
       "94           boards  \n",
       "95          unknown  \n",
       "96          apparel  \n",
       "97          unknown  \n",
       "98          apparel  \n",
       "\n",
       "[61 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from dateutil import parser  #A\n",
    "\n",
    "# Load the dataset  #B\n",
    "df = pd.read_csv(\"../setup/bills_surf_shop_orders.csv\")  #C\n",
    "\n",
    "# Standardize purchase_date to YYYY-MM-DD  #D\n",
    "def normalize_date(val):  #E\n",
    "    try:\n",
    "        return parser.parse(val).strftime('%Y-%m-%d')  #F\n",
    "    except Exception:\n",
    "        return None  #G\n",
    "\n",
    "df['purchase_date'] = df['purchase_date'].apply(normalize_date)  #H\n",
    "\n",
    "# Validate SKU format (3 uppercase letters + 3 digits)  #I\n",
    "df['SKU'] = df['SKU'].str.upper().str.extract(r'([A-Z]{3}\\d{3})', expand=False)  #J\n",
    "\n",
    "# Truncate product_description to 20 characters  #K\n",
    "df['description'] = df['description'].str[:20]  #L\n",
    "\n",
    "# Create full_name column from first and last names  #M\n",
    "df['full_name'] = df['first_name'].fillna('') + ' ' + df['last_name'].fillna('')  #N\n",
    "\n",
    "# Normalize store_location using mapping  #O\n",
    "location_map = {\n",
    "    'FL': 'Florida', 'Florida': 'Florida',\n",
    "    'NC': 'North Carolina', 'North Carolina': 'North Carolina',\n",
    "    'SC': 'South Carolina', 'South Carolina': 'South Carolina',\n",
    "    'NY': 'New York', 'New York': 'New York',\n",
    "    'NJ': 'New Jersey', 'New Jersey': 'New Jersey',\n",
    "    'MA': 'Massachusetts', 'Massachusetts': 'Massachusetts'\n",
    "}\n",
    "df['store_location'] = df['store_location'].map(location_map)  #P\n",
    "\n",
    "\n",
    "# Drop optional_note column  #Q\n",
    "df = df.drop(columns=['optional_note'], errors='ignore')  #R\n",
    "\n",
    "# Drop rows with null or invalid email format  #S\n",
    "email_pattern = r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"  #T\n",
    "df = df[df['email'].fillna('').str.match(email_pattern)]  #U\n",
    "\n",
    "# Filter out negative or missing purchase_amounts  #V\n",
    "df = df[df['purchase_amount'].fillna(0) > 0]  #W\n",
    "\n",
    "# Drop rows with missing age or customer_id  #X\n",
    "df = df.dropna(subset=['age', 'customer_id'])  #Y\n",
    "\n",
    "category_map = {\n",
    "    'Leash': 'accessories',\n",
    "    'Surf wax': 'accessories',\n",
    "    'Wax comb': 'accessories',\n",
    "    'Sunscreen': 'protection',\n",
    "    'Shortboard surfboard': 'boards',\n",
    "    'Longboard surfboard': 'boards',\n",
    "    'Soft-top surfboard': 'boards',\n",
    "    'Board shorts': 'apparel',\n",
    "    'Wetsuit - shorty': 'apparel',\n",
    "    'Wetsuit - full': 'apparel',\n",
    "    'Rash guard': 'apparel',\n",
    "    'Surf poncho': 'apparel',\n",
    "    'Fins': 'accessories',\n",
    "    'Traction pad': 'accessories',\n",
    "    'Beach towel': 'accessories',\n",
    "    'Dry bag': 'accessories',\n",
    "    'Roof rack straps': 'accessories',\n",
    "    'Ear plugs': 'protection',\n",
    "    'Surf helmet': 'protection',\n",
    "    'Waterproof watch': 'electronics'\n",
    "}\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df['product_category'] = df['description'].map(category_map).fillna('unknown')\n",
    "\n",
    "\n",
    "# Output cleaned DataFrame  #Z\n",
    "display(df)  #AA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd01af-b399-422f-8aa1-88ac4f74ad83",
   "metadata": {},
   "source": [
    "## 2 Clean the Dataset Using Chat Completions/Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cad2375c-01f8-4ed3-a8a3-341d2cdd7524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f282efab2a064c09a5b7ecfe6d60786d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning Records:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_date</th>\n",
       "      <th>cleaned_sku</th>\n",
       "      <th>truncated_description</th>\n",
       "      <th>full_name</th>\n",
       "      <th>standardized_location</th>\n",
       "      <th>filtered_email</th>\n",
       "      <th>filtered_purchase_amount</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>CPT535</td>\n",
       "      <td>Leash</td>\n",
       "      <td>Katie Smith</td>\n",
       "      <td>Florida</td>\n",
       "      <td>katie.smith@hotmail.com</td>\n",
       "      <td>516.46</td>\n",
       "      <td>accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>CUC263</td>\n",
       "      <td>Traction pad</td>\n",
       "      <td>Erika Mcdaniel</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>erika.mcdaniel@yahoo.com</td>\n",
       "      <td>0.00</td>\n",
       "      <td>accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>CPT535</td>\n",
       "      <td>Leash</td>\n",
       "      <td>Edward Hunt</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>edward.hunt@hotmail.com</td>\n",
       "      <td>0.00</td>\n",
       "      <td>accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>ZBA521</td>\n",
       "      <td>Surf wax</td>\n",
       "      <td>Amanda Shaw</td>\n",
       "      <td>New York</td>\n",
       "      <td>amanda.shaw@hotmail.com</td>\n",
       "      <td>40.93</td>\n",
       "      <td>accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>ICZ284</td>\n",
       "      <td>Wax comb</td>\n",
       "      <td>Betty Hicks</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>betty.hicks@hotmail.com</td>\n",
       "      <td>114.53</td>\n",
       "      <td>accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>.null</td>\n",
       "      <td>null</td>\n",
       "      <td>Edward Hunt</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>edward.hunt@hotmail.com</td>\n",
       "      <td>672.24</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>VVL444</td>\n",
       "      <td>Rash guard</td>\n",
       "      <td>Alicia Carroll</td>\n",
       "      <td>Florida</td>\n",
       "      <td>alicia.carroll@gmail.com</td>\n",
       "      <td>708.20</td>\n",
       "      <td>apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>/ull</td>\n",
       "      <td>/ull</td>\n",
       "      <td>Sarah Reeves</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>sarah.reeves@yahoo.com</td>\n",
       "      <td>544.23</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>AAS588</td>\n",
       "      <td>Surf poncho</td>\n",
       "      <td>Danielle Cooke</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>danielle.cooke@hotmail.com</td>\n",
       "      <td>656.46</td>\n",
       "      <td>apparel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>VYJ034</td>\n",
       "      <td>Fins</td>\n",
       "      <td>Brianna Brennan</td>\n",
       "      <td>Florida</td>\n",
       "      <td>brianna.brennan@yahoo.com</td>\n",
       "      <td>0.00</td>\n",
       "      <td>accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   normalized_date cleaned_sku truncated_description        full_name  \\\n",
       "0       2023-01-15      CPT535                 Leash      Katie Smith   \n",
       "1       2023-12-31      CUC263          Traction pad   Erika Mcdaniel   \n",
       "2       2023-01-15      CPT535                 Leash      Edward Hunt   \n",
       "3       2023-01-15      ZBA521              Surf wax      Amanda Shaw   \n",
       "4       2023-01-15      ICZ284              Wax comb      Betty Hicks   \n",
       "..             ...         ...                   ...              ...   \n",
       "95      2023-01-15       .null                  null      Edward Hunt   \n",
       "96      2023-12-31      VVL444            Rash guard   Alicia Carroll   \n",
       "97      2023-01-01        /ull                  /ull     Sarah Reeves   \n",
       "98      2023-01-01      AAS588           Surf poncho   Danielle Cooke   \n",
       "99      2023-12-31      VYJ034                  Fins  Brianna Brennan   \n",
       "\n",
       "   standardized_location              filtered_email  \\\n",
       "0                Florida     katie.smith@hotmail.com   \n",
       "1               Virginia    erika.mcdaniel@yahoo.com   \n",
       "2          Massachusetts     edward.hunt@hotmail.com   \n",
       "3               New York     amanda.shaw@hotmail.com   \n",
       "4          Massachusetts     betty.hicks@hotmail.com   \n",
       "..                   ...                         ...   \n",
       "95            New Jersey     edward.hunt@hotmail.com   \n",
       "96               Florida    alicia.carroll@gmail.com   \n",
       "97            New Jersey      sarah.reeves@yahoo.com   \n",
       "98              Virginia  danielle.cooke@hotmail.com   \n",
       "99               Florida   brianna.brennan@yahoo.com   \n",
       "\n",
       "    filtered_purchase_amount product_category  \n",
       "0                     516.46      accessories  \n",
       "1                       0.00      accessories  \n",
       "2                       0.00      accessories  \n",
       "3                      40.93      accessories  \n",
       "4                     114.53      accessories  \n",
       "..                       ...              ...  \n",
       "95                    672.24          unknown  \n",
       "96                    708.20          apparel  \n",
       "97                    544.23          unknown  \n",
       "98                    656.46          apparel  \n",
       "99                      0.00      accessories  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm  #A\n",
    "\n",
    "# Load API key from .env file  #B\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  #C\n",
    "\n",
    "# Define the data model for cleaned output  #D\n",
    "class SurfShopCleaning(BaseModel):  #E\n",
    "    normalized_date: Optional[str]  #F\n",
    "    cleaned_sku: Optional[str]  #G\n",
    "    truncated_description: Optional[str]  #H\n",
    "    full_name: str  #I\n",
    "    standardized_location: Optional[str]  #J\n",
    "    filtered_email: Optional[str]  #K\n",
    "    filtered_purchase_amount: Optional[float]  #L\n",
    "    product_category: Optional[str]  #M\n",
    "\n",
    "# Load the raw dataset  #N\n",
    "df = pd.read_csv(\"../setup/bills_surf_shop_orders.csv\")  #O\n",
    "records = df.to_dict(orient=\"records\")  #P\n",
    "\n",
    "# Create an empty list to collect cleaned rows  #Q\n",
    "cleaned_rows = []  #R\n",
    "\n",
    "# Loop through each record using tqdm for a progress bar  #S\n",
    "for record in tqdm(records, desc=\"Cleaning Records\"):  #T\n",
    "    row_prompt = (\n",
    "        \"You are a data cleaning assistant. Clean this single record and return:\\n\"\n",
    "        \"- normalized_date: Convert purchase_date to YYYY-MM-DD or null\\n\"\n",
    "        \"- cleaned_sku: Valid 3-uppercase-letter + 3-digit SKU or null\\n\"\n",
    "        \"- truncated_description: First 20 characters of description or null\\n\"\n",
    "        \"- full_name: Combine first_name and last_name\\n\"\n",
    "        \"- standardized_location: Normalize store_location to full name or null\\n\"\n",
    "        \"- filtered_email: Return only if valid format, else null\\n\"\n",
    "        \"- filtered_purchase_amount: Return only if positive and non-null, else null\\n\"\n",
    "        \"- product_category: Map description to one of: boards, apparel, protection, electronics, accessories. Use 'unknown' if unclear.\\n\"\n",
    "        \"Return the result as a JSON object matching the SurfShopCleaning structure.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Make the API call  #U\n",
    "        completion = openai.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": row_prompt},  #V\n",
    "                {\"role\": \"user\", \"content\": str(record)}  #W\n",
    "            ],\n",
    "            response_format=SurfShopCleaning  #X\n",
    "        )\n",
    "\n",
    "        cleaned = completion.choices[0].message.parsed.dict()  #Y\n",
    "        cleaned_rows.append(cleaned)  #Z\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {record['order_id']}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame  #AA\n",
    "df_cleaned = pd.DataFrame(cleaned_rows)  #AB\n",
    "\n",
    "# Drop rows missing critical fields  #AC\n",
    "df_cleaned = df_cleaned.dropna(subset=[\"filtered_email\", \"filtered_purchase_amount\"])  #AD\n",
    "\n",
    "# Display the cleaned dataset  #AE\n",
    "display(df_cleaned)  #AF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb404586-4a87-4da2-b13d-c7c6d0e01ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
