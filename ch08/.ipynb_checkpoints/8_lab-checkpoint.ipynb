{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 8 Lab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Lab: Building a Multi-Agent News Pipeline for Disney Analytics\n",
        "\n",
        "Welcome to the Magic Kingdom of data pipelines! Disney's corporate strategy team needs real-time intelligence on how the company is covered in the news. They want to track sentiment, identify which business lines are being discussed (Parks, Movies, Streaming, Merchandise), and understand regional coverage patterns.\n",
        "\n",
        "Your mission: Build an AI-powered news extraction and transformation pipeline that can:\n",
        "- Extract Disney news articles from NewsAPI\n",
        "- Use specialized AI agents to transform and enrich the data\n",
        "- Categorize articles by Disney business line\n",
        "- Handle timezone conversions and topic classification\n",
        "- Generate database schemas and load data to Postgres\n",
        "\n",
        "This lab mirrors real-world multi-agent systems where different AI specialists handle extraction, sentiment analysis, quality checks, and schema generation.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Extract: Building the News Extraction Agent\n",
        "\n",
        "**Scenario:** Disney's strategy team needs fresh news coverage from the last 24 hours. They want the raw JSON data from NewsAPI so they can process it with AI agents.\n",
        "\n",
        "**Goal:** Create a function that extracts Disney news articles and loads them into a DataFrame.\n",
        "\n",
        "**Input:** NewsAPI endpoint with Disney query\n",
        "\n",
        "**Output:** DataFrame with raw article JSON blobs\n",
        "\n",
        "**Task:** \n",
        "- Set up NewsAPI connection\n",
        "- Extract articles from yesterday to today\n",
        "- Store full JSON in a single column DataFrame\n",
        "\n",
        "‚úÖ **Try It Now:** Change the query to search for \"Disney Parks\" or \"Disney+\" specifically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from datetime import datetime, timedelta\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "NEWS_API_KEY = os.getenv(\"NEWS_API_KEY\")\n",
        "\n",
        "# Dynamic date calculation: today minus one day\n",
        "today = datetime.now().date()\n",
        "yesterday = today - timedelta(days=1)\n",
        "\n",
        "# Function to extract articles from NewsAPI\n",
        "def extract_articles(query, from_date=yesterday, api_key=NEWS_API_KEY):\n",
        "    url = f'https://newsapi.org/v2/everything?q={query}&from={from_date}&to={today}&apiKey={api_key}'\n",
        "    response = requests.get(url)\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        articles = response.json().get('articles', [])\n",
        "        logging.info(f\"Successfully extracted {len(articles)} articles.\")\n",
        "        return articles\n",
        "    else:\n",
        "        logging.error(f\"Failed to fetch articles. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Extract Disney news\n",
        "articles = extract_articles('Disney')\n",
        "\n",
        "# Build a DataFrame with one row per article and full JSON blob\n",
        "df = pd.DataFrame({'article': articles})\n",
        "print(f\"Extracted {len(df)} Disney news articles\")\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Transform: Structured Data Extraction with AI Agents\n",
        "\n",
        "**Scenario:** The raw JSON blobs are hard to work with. Disney's team needs clean, structured data extracted from each article.\n",
        "\n",
        "**Goal:** Use AI with structured outputs to extract key fields and perform sentiment analysis.\n",
        "\n",
        "**Input:** Raw article JSON from Step 1\n",
        "\n",
        "**AI Task:** Extract source, title, summary, publish date, and analyze sentiment\n",
        "\n",
        "**Output:** Clean DataFrame with structured fields\n",
        "\n",
        "**Task:**\n",
        "- Define a Pydantic schema for extracted articles\n",
        "- Create an extraction agent with clear instructions\n",
        "- Add a sentiment analysis agent to score each article\n",
        "\n",
        "‚úÖ **Try It Now:** Modify the sentiment scale to be 1-5 stars instead of -1 to 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import openai\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel\n",
        "\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Define extraction schema\n",
        "class ExtractedArticle(BaseModel):\n",
        "    source: str\n",
        "    title: str\n",
        "    short_summary: str\n",
        "    publish_date: str\n",
        "\n",
        "# System prompt for extraction agent\n",
        "system_prompt = f\"\"\"\n",
        "You are a data extraction agent. For each input article JSON, return a single object matching this schema:\n",
        "{ExtractedArticle.schema_json(indent=2)}\n",
        "\n",
        "Use the raw JSON to guide extraction with natural language hints:\n",
        "- source: use article['source']['name'] when present.\n",
        "- title: use article['title'].\n",
        "- short_summary: 1‚Äì2 sentences summarizing the article in plain English.\n",
        "- publish_date: use article['publishedAt'] (ISO-8601 timestamp).\n",
        "\n",
        "Return exactly one object that matches the schema.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Sentiment analysis agent\n",
        "def perform_sentiment_analysis(text: str):\n",
        "    \"\"\"Analyze sentiment of article summary.\"\"\"\n",
        "    prompt = (\n",
        "        \"Analyze the sentiment of the following text and return a numerical sentiment \"\n",
        "        \"score from -1 (very negative) to 1 (very positive). Return only the number: \"\n",
        "        f\"{text}\"\n",
        "    )\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=50,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        sentiment_str = response.choices[0].message.content.strip()\n",
        "        return float(sentiment_str)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error performing sentiment analysis: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process articles with extraction and sentiment agents\n",
        "results = []\n",
        "input_articles = articles  # from prior cell\n",
        "\n",
        "# Limit to first 5 for quick iteration; adjust as needed\n",
        "for idx, article in enumerate(input_articles[:5]):\n",
        "    try:\n",
        "        # Extraction agent\n",
        "        completion = openai.beta.chat.completions.parse(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": f\"{article}\"}\n",
        "            ],\n",
        "            response_format=ExtractedArticle\n",
        "        )\n",
        "        parsed = completion.choices[0].message.parsed\n",
        "        if parsed:\n",
        "            item = parsed.dict()\n",
        "            # Sentiment agent\n",
        "            item[\"sentiment\"] = perform_sentiment_analysis(item[\"short_summary\"])\n",
        "            results.append(item)\n",
        "    except Exception as e:\n",
        "        print(f\"Error on article {idx}: {e}\")\n",
        "\n",
        "extracted_df = pd.DataFrame(results)\n",
        "print(f\"\\nExtracted and analyzed {len(extracted_df)} articles:\")\n",
        "extracted_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Schema Generation: AI-Powered DDL Creation\n",
        "\n",
        "**Scenario:** Disney's data engineering team needs to store this enriched data in Postgres, but they don't want to manually write DDL statements every time the schema changes.\n",
        "\n",
        "**Goal:** Use AI to generate the CREATE TABLE DDL based on the DataFrame schema.\n",
        "\n",
        "**Input:** Field names and types from enriched_df\n",
        "\n",
        "**AI Task:** Generate production-ready PostgreSQL DDL\n",
        "\n",
        "**Output:** Valid CREATE TABLE statement\n",
        "\n",
        "**Task:**\n",
        "- Define expected column types\n",
        "- Create DDL generation agent\n",
        "- Generate CREATE TABLE statement\n",
        "- Add surrogate key and audit columns\n",
        "\n",
        "‚úÖ **Try It Now:** Ask the AI to add indexes on commonly queried columns (topic, region, business_line).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Enrich: Quality, Categorization & Disney Business Line Agent\n",
        "\n",
        "**Scenario:** Disney's team needs articles categorized by topic, region, timezone, AND which business line the news relates to. This is critical for routing insights to the right executives.\n",
        "\n",
        "**Goal:** Build a quality & categorization agent that adds:\n",
        "- Multiple timezone conversions (EST, PST, GMT)\n",
        "- Topic classification (Financial, Product/Technology, etc.)\n",
        "- Region detection (North America, Europe, Asia, etc.)\n",
        "- **Disney Business Line** (Parks, Movies, Streaming/Disney+, Merchandise, Cruise Line, ESPN/Sports, Corporate/Other)\n",
        "\n",
        "**Input:** Extracted articles from Step 2\n",
        "\n",
        "**AI Task:** Enrich each article with quality checks and Disney-specific categorization\n",
        "\n",
        "**Output:** Fully enriched DataFrame ready for database loading\n",
        "\n",
        "**Task:**\n",
        "- Define Pydantic schema with all enrichment fields\n",
        "- Create quality/categorization agent prompt\n",
        "- Add Disney business line detection logic\n",
        "- Process all articles through the agent\n",
        "\n",
        "‚úÖ **Try It Now:** Add a new business line category for \"Theme Park Technology\" or \"Imagineering\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import openai\n",
        "import pandas as pd\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "\n",
        "# Define enrichment schema with Disney business line\n",
        "class QualityCategorization(BaseModel):\n",
        "    short_date: str            # YYYY-MM-DD (no timezone)\n",
        "    publish_est: str           # ISO-8601 datetime in America/New_York\n",
        "    publish_pst: str           # ISO-8601 datetime in America/Los_Angeles\n",
        "    publish_gmt: str           # ISO-8601 datetime in GMT/UTC (+00:00)\n",
        "    topic: str                 # One of: Financial, Operations, Product/Technology, Regulatory/Legal, Market/Competition, Executive/Personnel, Strategy/M&A, Customers/Partnerships, Supply Chain/Manufacturing, ESG/Sustainability, Risk/Incidents, Marketing/PR\n",
        "    region: str                # One of: North America, South America, Europe, Africa, Middle East, Asia, Oceania\n",
        "    business_line: str         # Disney-specific: Parks, Movies, Streaming/Disney+, Merchandise, Cruise Line, ESPN/Sports, Corporate/Other\n",
        "\n",
        "# System prompt for quality/categorization agent\n",
        "qc_system_prompt = f\"\"\"\n",
        "You are a data quality and categorization agent specializing in Disney corporate intelligence. For each input article, return a single object matching this schema:\n",
        "{QualityCategorization.schema_json(indent=2)}\n",
        "\n",
        "Instructions:\n",
        "- short_date: Derive from the input publish_date by dropping time and timezone, format as YYYY-MM-DD.\n",
        "- publish_est / publish_pst / publish_gmt: Convert the input publish_date to the specified timezone and return ISO-8601 (include timezone offset). Use the original timestamp as ground truth.\n",
        "- topic: Choose the best label from [Financial, Operations, Product/Technology, Regulatory/Legal, Market/Competition, Executive/Personnel, Strategy/M&A, Customers/Partnerships, Supply Chain/Manufacturing, ESG/Sustainability, Risk/Incidents, Marketing/PR]. Pick the closest match.\n",
        "- region: Infer using language cues, source, and content (country/city mentions). Map to one of:\n",
        "  [North America, South America, Europe, Africa, Middle East, Asia, Oceania]. Always use exactly these labels.\n",
        "- business_line: Determine which Disney business unit the article primarily discusses:\n",
        "  * Parks - Theme parks, resorts, attractions, park experiences, Walt Disney World, Disneyland\n",
        "  * Movies - Film releases, box office, theatrical content, studios, Pixar, Marvel, Lucasfilm\n",
        "  * Streaming/Disney+ - Disney+, Hulu, streaming services, original series, content strategy\n",
        "  * Merchandise - Consumer products, toys, licensing, retail\n",
        "  * Cruise Line - Disney Cruise Line operations and ships\n",
        "  * ESPN/Sports - ESPN, sports broadcasting, sports content, sports rights\n",
        "  * Corporate/Other - General corporate news, earnings, leadership, multiple divisions, stock performance\n",
        "  \n",
        "Return strictly valid JSON with exactly these keys and no extra text.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Process articles through quality/categorization agent\n",
        "qc_results = []\n",
        "\n",
        "for idx, row in extracted_df.iterrows():\n",
        "    article_input = {\n",
        "        \"source\": row.get(\"source\", \"\"),\n",
        "        \"title\": row.get(\"title\", \"\"),\n",
        "        \"short_summary\": row.get(\"short_summary\", \"\"),\n",
        "        \"publish_date\": row.get(\"publish_date\", \"\")\n",
        "    }\n",
        "    try:\n",
        "        completion = openai.beta.chat.completions.parse(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": qc_system_prompt},\n",
        "                {\"role\": \"user\", \"content\": f\"{article_input}\"}\n",
        "            ],\n",
        "            response_format=QualityCategorization\n",
        "        )\n",
        "        parsed = completion.choices[0].message.parsed\n",
        "        if parsed:\n",
        "            qc_results.append(parsed.dict())\n",
        "        else:\n",
        "            qc_results.append({\n",
        "                \"short_date\": \"\",\n",
        "                \"publish_est\": \"\",\n",
        "                \"publish_pst\": \"\",\n",
        "                \"publish_gmt\": \"\",\n",
        "                \"topic\": \"\",\n",
        "                \"region\": \"\",\n",
        "                \"business_line\": \"\"\n",
        "            })\n",
        "    except Exception as e:\n",
        "        logging.error(f\"QC error on row {idx}: {e}\")\n",
        "        qc_results.append({\n",
        "            \"short_date\": \"\",\n",
        "            \"publish_est\": \"\",\n",
        "            \"publish_pst\": \"\",\n",
        "            \"publish_gmt\": \"\",\n",
        "            \"topic\": \"\",\n",
        "            \"region\": \"\",\n",
        "            \"business_line\": \"\"\n",
        "        })\n",
        "\n",
        "# Combine extracted data with quality/categorization results\n",
        "qc_df = pd.DataFrame(qc_results)\n",
        "enriched_df = pd.concat([extracted_df.reset_index(drop=True), qc_df], axis=1)\n",
        "\n",
        "print(f\"\\n‚ú® Enriched {len(enriched_df)} articles with quality checks and Disney business line:\")\n",
        "enriched_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "import openai\n",
        "from pydantic import BaseModel\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Pydantic for DDL contract\n",
        "class TableDDL(BaseModel):\n",
        "    ddl: str  # CREATE TABLE ... statement only\n",
        "\n",
        "# Model-friendly schema of enriched_df\n",
        "sample_fields = {\n",
        "    \"source\": \"text\",\n",
        "    \"title\": \"text\",\n",
        "    \"short_summary\": \"text\",\n",
        "    \"publish_date\": \"timestamptz\",\n",
        "    \"sentiment\": \"numeric\",\n",
        "    \"short_date\": \"date\",\n",
        "    \"publish_est\": \"timestamptz\",\n",
        "    \"publish_pst\": \"timestamptz\",\n",
        "    \"publish_gmt\": \"timestamptz\",\n",
        "    \"topic\": \"text\",\n",
        "    \"region\": \"text\",\n",
        "    \"business_line\": \"text\"\n",
        "}\n",
        "\n",
        "# Compose prompt to generate DDL\n",
        "ddl_prompt = f\"\"\"\n",
        "You are a SQL DDL assistant. Return only a single valid PostgreSQL CREATE TABLE statement for table name disney_news_articles.\n",
        "Use these columns and suggested types. Adjust types conservatively if needed, add NOT NULL only if obviously safe.\n",
        "Columns:\n",
        "{json.dumps(sample_fields, indent=2)}\n",
        "\n",
        "Rules:\n",
        "- Include a surrogate primary key id BIGSERIAL PRIMARY KEY.\n",
        "- Add created_at TIMESTAMPTZ DEFAULT NOW().\n",
        "- Use snake_case column names exactly as provided.\n",
        "- Return strictly the SQL, no comments or extra text.\n",
        "\"\"\".strip()\n",
        "\n",
        "# Ask AI for DDL\n",
        "completion = openai.beta.chat.completions.parse(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": ddl_prompt},\n",
        "        {\"role\": \"user\", \"content\": \"Generate the DDL now.\"}\n",
        "    ],\n",
        "    response_format=TableDDL\n",
        ")\n",
        "TABLE_DDL = completion.choices[0].message.parsed.ddl\n",
        "\n",
        "print(\"Generated DDL:\")\n",
        "print(TABLE_DDL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Load: Write Enriched Data to Postgres\n",
        "\n",
        "**Scenario:** Now that we have clean, enriched data and a database schema, let's load everything into Postgres for Disney's analytics team.\n",
        "\n",
        "**Goal:** Create the table and insert all enriched articles.\n",
        "\n",
        "**Input:** Generated DDL + enriched DataFrame\n",
        "\n",
        "**Output:** Data loaded in Postgres\n",
        "\n",
        "**Task:**\n",
        "- Connect to Postgres database\n",
        "- Execute DDL to create table\n",
        "- Batch insert enriched articles\n",
        "- Verify data loaded successfully\n",
        "\n",
        "‚úÖ **Try It Now:** Add an upsert mechanism to prevent duplicate articles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Ensure psycopg is available\n",
        "try:\n",
        "    import psycopg\n",
        "except Exception:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"psycopg[binary]>=3.1\"], check=False)\n",
        "    import psycopg\n",
        "\n",
        "# Connect to Postgres\n",
        "conn = psycopg.connect(\n",
        "    host=os.getenv(\"PGHOST\", \"localhost\"),\n",
        "    port=os.getenv(\"PGPORT\", \"5432\"),\n",
        "    dbname=os.getenv(\"PGDATABASE\", \"news_db\"),\n",
        "    user=os.getenv(\"PGUSER\", \"news_user\"),\n",
        "    password=os.getenv(\"PGPASSWORD\", \"\")\n",
        ")\n",
        "\n",
        "# Create table if not exists (idempotent)\n",
        "with conn.cursor() as cur:\n",
        "    try:\n",
        "        cur.execute(TABLE_DDL)\n",
        "        print(\"‚úÖ Table created successfully\")\n",
        "    except Exception as e:\n",
        "        # If table already exists, ignore\n",
        "        msg = str(e).lower()\n",
        "        if \"already exists\" not in msg:\n",
        "            raise\n",
        "        print(\"‚ÑπÔ∏è  Table already exists\")\n",
        "conn.commit()\n",
        "\n",
        "# Prepare insert statement\n",
        "cols = [\n",
        "    \"source\", \"title\", \"short_summary\", \"publish_date\", \"sentiment\",\n",
        "    \"short_date\", \"publish_est\", \"publish_pst\", \"publish_gmt\", \n",
        "    \"topic\", \"region\", \"business_line\"\n",
        "]\n",
        "\n",
        "placeholders = \",\".join([\"%s\"] * len(cols))\n",
        "insert_sql = f\"INSERT INTO disney_news_articles ({','.join(cols)}) VALUES ({placeholders})\"\n",
        "\n",
        "# Convert dataframe rows to tuples\n",
        "rows = []\n",
        "for _, r in enriched_df.iterrows():\n",
        "    rows.append(tuple(r.get(c) for c in cols))\n",
        "\n",
        "# Batch insert\n",
        "with conn.cursor() as cur:\n",
        "    if rows:\n",
        "        cur.executemany(insert_sql, rows)\n",
        "        print(f\"‚úÖ Inserted {len(rows)} rows into disney_news_articles\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No rows to insert\")\n",
        "conn.commit()\n",
        "\n",
        "conn.close()\n",
        "\n",
        "print(\"\\nüéâ Pipeline complete! Disney news articles loaded to Postgres.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Verify: Query the Database\n",
        "\n",
        "**Scenario:** Let's verify that everything loaded correctly and explore the Disney business line distribution.\n",
        "\n",
        "**Goal:** Query the database to see our enriched articles and analyze business line coverage.\n",
        "\n",
        "**Task:**\n",
        "- Query total row count\n",
        "- Preview the latest articles\n",
        "- Analyze articles by business line and sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import psycopg\n",
        "\n",
        "# Reconnect to database\n",
        "conn = psycopg.connect(\n",
        "    host=os.getenv(\"PGHOST\", \"localhost\"),\n",
        "    port=os.getenv(\"PGPORT\", \"5432\"),\n",
        "    dbname=os.getenv(\"PGDATABASE\", \"news_db\"),\n",
        "    user=os.getenv(\"PGUSER\", \"news_user\"),\n",
        "    password=os.getenv(\"PGPASSWORD\", \"\")\n",
        ")\n",
        "\n",
        "# Show total rows\n",
        "with conn.cursor() as cur:\n",
        "    cur.execute(\"SELECT COUNT(*) FROM disney_news_articles;\")\n",
        "    total_rows = cur.fetchone()[0]\n",
        "print(f\"üìä Total articles in database: {total_rows}\")\n",
        "\n",
        "# Preview last 5 rows\n",
        "with conn.cursor() as cur:\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT id, source, title, publish_date, topic, region, business_line, sentiment, created_at\n",
        "        FROM disney_news_articles\n",
        "        ORDER BY id DESC\n",
        "        LIMIT 5;\n",
        "        \"\"\"\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    cols = [c[0] for c in cur.description]\n",
        "\n",
        "preview_df = pd.DataFrame(rows, columns=cols)\n",
        "print(\"\\nüì∞ Latest articles:\")\n",
        "display(preview_df)\n",
        "\n",
        "# Analyze by business line\n",
        "with conn.cursor() as cur:\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT \n",
        "            business_line,\n",
        "            COUNT(*) as article_count,\n",
        "            ROUND(AVG(sentiment)::numeric, 2) as avg_sentiment\n",
        "        FROM disney_news_articles\n",
        "        GROUP BY business_line\n",
        "        ORDER BY article_count DESC;\n",
        "        \"\"\"\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "    cols = [c[0] for c in cur.description]\n",
        "\n",
        "conn.close()\n",
        "\n",
        "business_line_df = pd.DataFrame(rows, columns=cols)\n",
        "print(\"\\nüè∞ Articles by Disney Business Line:\")\n",
        "display(business_line_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üí¨ Discussion Questions\n",
        "\n",
        "Now that you've built a complete multi-agent news pipeline for Disney, take a moment to reflect:\n",
        "\n",
        "### **Multi-Agent Architecture**\n",
        "* How did breaking the pipeline into specialized agents (extraction, sentiment, categorization, DDL) improve code organization?\n",
        "* What are the benefits of having separate agents vs. one large prompt that does everything?\n",
        "* How would you handle agent failures or rate limiting in production?\n",
        "* Could you parallelize some of these agents for better performance?\n",
        "\n",
        "### **Disney Business Line Classification**\n",
        "* How accurate was the AI at identifying Disney business lines?\n",
        "* What edge cases did you notice (e.g., articles about Disney+ movies)?\n",
        "* How would you validate the business_line classifications in production?\n",
        "* Should some articles belong to multiple business lines?\n",
        "\n",
        "### **Schema Evolution & Data Quality**\n",
        "* What happens when you add a new field to the pipeline?\n",
        "* How would you handle schema migrations in production?\n",
        "* What data quality checks would you add (duplicates, invalid dates, etc.)?\n",
        "* How would you monitor AI agent accuracy over time?\n",
        "\n",
        "### **Production Considerations**\n",
        "* How would you schedule this pipeline to run daily?\n",
        "* What error handling and retry logic would you add?\n",
        "* How would you handle API rate limits from NewsAPI and OpenAI?\n",
        "* What monitoring and alerting would you implement?\n",
        "\n",
        "### **Try It Now: Advanced Challenges**\n",
        "\n",
        "1. **Add a new business line** like \"Theme Park Technology\" or \"Imagineering\" and update the categorization agent.\n",
        "\n",
        "2. **Create a duplicate detection agent** that checks if an article already exists in the database before inserting.\n",
        "\n",
        "3. **Build a summary agent** that creates executive-level summaries by business line (e.g., \"This week in Disney Parks: 3 positive articles about new attractions...\").\n",
        "\n",
        "4. **Add a competitor analysis agent** that flags when articles mention Disney competitors (Universal, Warner Bros, Netflix, etc.).\n",
        "\n",
        "5. **Implement a data quality dashboard** that shows:\n",
        "   - Articles by business line over time\n",
        "   - Sentiment trends by region\n",
        "   - Topic distribution\n",
        "   - Data completeness metrics\n",
        "\n",
        "6. **Create an alert agent** that sends notifications when:\n",
        "   - Negative sentiment articles appear about Disney Parks\n",
        "   - Financial news is published\n",
        "   - New executive/personnel changes are announced\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Congratulations!** You've built a production-grade multi-agent news pipeline with AI-powered extraction, transformation, categorization, and loading. You've seen how specialized agents can handle complex data engineering tasks while maintaining flexibility and code clarity. The Disney business line classification demonstrates how AI can add domain-specific intelligence that would be difficult to achieve with traditional rule-based systems.\n",
        "\n",
        "**Next Steps:**\n",
        "- Explore orchestration patterns for running this pipeline on a schedule\n",
        "- Learn about monitoring and observability for AI-powered pipelines\n",
        "- Discover how to version and test AI agents\n",
        "- Build dashboards that surface these insights to business stakeholders\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
